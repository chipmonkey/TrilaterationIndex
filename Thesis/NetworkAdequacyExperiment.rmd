---
output:
  pdf_document: default
  html_document: default
---

## Multilateration NA Algorithms
Recalling our definition of [Network Adequacy](#Network-Adequacy) (NA), we look to leverage the Multilateration Index to improve performance when computationally answering NA questions.

In SQL, we implement two $NA$ algorithms to compare this theoretical setup to a typical real-world example detecting whether there exists a record in $P$ within $d$ of each of a set of query points $Q$:

* "NAIVE-NA" - the default SQL Query algorithm
* "TRILAT-NA" - the approach we've described

### NAIVE-NA
The most basic SQL query, in a database that has Geospatial extensions, to calculate Network Adequacy is something like:

\singlespacing

```{SQL naive_na, attr.source='.numberLines'}
    select count(q.sampleid) as qcount,
        count(p2.sampleid) as tcount
    from q_points q
    left join lateral (select p.sampleid from p_points p
        where
          st_distance(p.st_geompoint, q.st_geompoint) <= (1609.34 * mydist)
        limit 1
    ) p2 on true
```

\doublespacing

This assumes two tables - "q_points" and "p_points" containing the points in $Q$ and $P$ respectively.  Each has a field "st_geompoint" containing a geospatial position for each point.  The "st_distance" function is a SQL function to calculate the distance between two points - in our case we need to ensure the database uses the accurate Geodesic calculation from our research.[@Karney2013]

This returns the number of records in $Q$ as $qcount$ and the number of records in $P$ as $tcount$.  The Network Adequacy Percent is then $\frac{tcount}{qcount}$.

Note that this function is in PostgreSQL syntax; it requires slight moderation but otherwise works (we tested) in Microsoft SQL Server and MySQL.  It likely works with little modification in any database which supports the SQL:1999 standard for lateral joins and geospatial points and distance functions.  One thing that is NOT identical between database implementations is the ability or effectiveness of database indexes on this query.  In PostgreSQL, we have experimented and found that 

Also note the $1609.34$ - this is to convert the distance from meters to miles, which is not core to the algorithm, but left here since these are the units we work with in our experimental results, and as an example.

\newpage
### TRILAT-NA

Recall that we require fixed reference points for Trilateration, and per our previous construction, we selected these:

* Point 1: $90.000000, 0.000000$ (The geographic north pole) 
* Point 2: $38.260000, -85.760000$ (Louisville, KY on the Ohio River) 
* Point 3: $-19.22000, 159.93000$ (Sandy Island, New Caledonia)

The "NAIVE-NA" query requires no real setup, other than storing the data from the $P$ and $Q$ data sets.  Not so here -- we require additional fields added to the database in the q_points and p_points tables to store the distances from each point to these reference points.  We name those fields $refdist1$, $refdist2$, and $refdist3$.  Recall that this is a one-time setup requiring $3*|P|+3*|Q|$ calls to st_distance.

The SQL implementation of the TRILAT-NA algorithm then looks like this:

\singlespacing

```{SQL trilat_na, attr.source='.numberLines'}
    select count(q.sampleid) as qcount,
        count(p2.sampleid) as tcount
    from q_points q
    left join lateral (select p.sampleid from p_points p
        where
          abs(q.refdist1 - p.refdist1) <= (1609.34 * mydist)
          and abs(q.refdist2 - p.refdist2) <= (1609.34 * mydist)
          and abs(q.refdist3 - p.refdist3) <= (1609.34 * mydist)
          and st_distance(p.st_geompoint, q.st_geompoint) <= (1609.34 * mydist)
        limit 1
    ) p2 on true
```

\doublespacing

Note that this is identical to the query for NAIVE-NA, with the addition of the three lines comparing the $refdist$ values.

These accomplish two things:

1.  They allow SQL to optimize using normal (non-geospatial) database indexes when comparing between the two points $p$ and $q$, using simple subtraction rather than a complicated geodesic query.  
2.  They allow for three opportunities to reduce the data set size before the high cost geodesic query is performed.  Recall our figure [Monte Carlo Estimating Ring Overlap Area] that exhibited how the area of the three overlapping rings of width $d$ was $<<$ the area of the search space; a similar thing happens here... For a given distance $d$, the set of points where $q.refdist_i - p.refdist_i <= d$, is the intersection of those three rings of width $d$ with centers on the three reference points and with diameters such that the middle of each ring passes through $q$.  This eliminates most points if $d$ is relatively small - small enough that some points $q$ are inadequate is generally a good test.

See the [Experimentation] and [Experimental Results] sections for details on our specific implementations, tests, and results.