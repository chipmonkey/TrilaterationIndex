---
title: "SQL_Results"
author: "Chip Lynch"
date: "4/3/2021"
output:
  pdf_document: default
  html_document: default
---
\pagebreak

## SQL Network Adequacy Results

We conducted our experiment as described, resulting in 36 combinations of point ($P$) and query ($Q$) sets from the same 150,000 point sample we've used for $NN$ experiments.  The data sets are categorized and range from 100 to 32,000 points, as shown here:

\singlespacing

```{r category_counts_sql, echo=FALSE}
library(knitr)
library(data.table)

data <- read.csv('../postgresql/query_timings.csv')
data <- data[data$notes %like% 'Adequ',]

names(data)[3] <- 'timing_s'

cat_data <- unique(data[,c('leftcat', 'leftcount')])
names(cat_data) <- c('category', 'record count')
kable(cat_data, caption="Record counts by category used in experiment")
```

\doublespacing

### Result Charts

Recall that we are calculating the Network Adequacy Percent (NAP), which requires a distance $d$, and we performed experiments with $d \in (0.1, 1, 10, 100)$.  At a high level, our TRILAT-NA algorithm performs well against the naive algorithm overall.  This initial density box-plot shows the Trilateration performance against the st_distance function overall.  Note that this figure is on a log scale - some of the Trilateration improvements are 100 times faster in the best case, and overall notably faster in aggregate:

```{r QP_size_comparison_boxall, echo=FALSE, message=FALSE, fig.cap="Overall Timings By Category (s) - log10 scale - lower is better", fig.width=10, fig.height=4}
library(ggplot2)
library(data.table)
library(ggdist)

data <- read.csv('../postgresql/query_timings.csv')
names(data)[3] <- 'timing_s'

data$dist <- 0
data[data$notes %like% 'dist: 0.1','dist'] <- 0.1
data[data$notes %like% 'dist: 1','dist'] <- 1
data[data$notes %like% 'dist: 10','dist'] <- 10
data[data$notes %like% 'dist: 100','dist'] <- 100

data <- data[data$notes %like% 'Adequ',]
data$algorithm = 'None'
data[data$notes %like% 'CatAdequacy','algorithm'] = 'st_distance'
data[data$notes %like% 'CatTrilatAdequacy', 'algorithm'] = 'Trilateration'
data <- data[data$algorithm != 'None',]
data$dist <- as.factor(data$dist)


library('tidyverse')
data$leftcat <- as.factor(data$leftcat)
data$rightcat <- as.factor(data$rightcat)

data$leftcount <- as.factor(data$leftcount)
data$rightcount <- as.factor(data$rightcount)

plotdata <- aggregate(data$timing_s,
          by = list(data$leftcount, data$rightcount, data$algorithm, data$dist),
          FUN=median)
names(plotdata) <- c('leftcount', 'rightcount', 'algorithm', 'dist', 'timing_s')

# Thank you https://www.r-bloggers.com/2021/07/ggdist-make-a-raincloud-plot-to-visualize-distribution-in-ggplot2/#google_vignette
boxp <- ggplot(data=plotdata, aes(x=algorithm, y=timing_s, fill=algorithm)) +
  geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=TRUE, width=0.25, alpha=0.5) +
  ggdist::stat_halfeye(adjust = 0.2, justification = -.2,
    .width = 0, point_color = NA
  ) +
  xlab('Timing in seconds') +
  scale_y_continuous(trans='log10') +
  coord_flip()
boxp
```

\newpage
It turns out that the relative performance of these queries depends strongly on the value of $d$, and the resulting NAP that $d$ implies.  Charts of our results against $d$ show this impact quite strongly:  

```{r sql_results, echo=FALSE, fig.cap="SQL Timings (s) relative to dist (d) - lower is better", fig.width=6, fig.height=3}
library(data.table)
library(ggplot2)

data <- read.csv('../postgresql/query_timings.csv')
names(data)[3] <- 'timing_s'

data$dist <- 0
data[data$notes %like% 'dist: 0.1','dist'] <- 0.1
data[data$notes %like% 'dist: 1','dist'] <- 1
data[data$notes %like% 'dist: 10','dist'] <- 10
data[data$notes %like% 'dist: 100','dist'] <- 100

data <- data[data$notes %like% 'Adequ',]
data$algorithm = 'None'
data[data$notes %like% 'CatAdequacy','algorithm'] = 'st_distance'
data[data$notes %like% 'CatTrilatAdequacy', 'algorithm'] = 'Trilateration'
# table(data$algorithm)
data <- data[data$algorithm != 'None',]
data$dist <- as.factor(data$dist)

ggplot(data=data, aes(x=dist, y=timing_s, fill=algorithm)) +
    geom_bar(stat='identity', position=position_dodge()) +
    scale_fill_manual(values=gray.colors(4))

```


It's worth noting that the actual NAP for $d$ varies in this data set; using $|P|=32000$ and $|Q|=10000$ we find:

\singlespacing

```{r d_NAP_chart, echo=FALSE}
library(data.table)
library(knitr)
dnap <- data.table(d=c(0.1,1,10,100),
                   NAP=c('42%', '88%', '98%', '100%'))
kable(dnap, caption="d vs. ~NAP")
```

\doublespacing

Of course this varies with the size of the data set, but the rough bounds are important.  The data set comprises points in Kentucky which is roughly 400 miles east-west and 200 miles north-south.  When $d$ is 100 miles, even with 100 points (our smallest sample), 100% coverage is all but guaranteed.  At 10 miles, we see some holes in coverage, but not many -- thus the 98% value.  When we get down to the one mile and one tenth of a mile ranges, coverage drop precipitously.  Even with our largest two categories, NAP is only 88% at 1 mile and 42% at 0.1 miles.  If we had been using real world data, imagine that we ask the question "what percent of people live within 1 mile of a pharmacy" to get an idea what these numbers could mean (if this were not synthetic data).

These percentages are akin to those we need to answer the healthcare questions set forth in the introduction.  For example to answer "80% of members under the age of 16 must live within 25 miles of a covered pediatrician" we would set d=25 miles, and identify the sets of points for "members under the age of 16", and "covered pediatrician" - those sets could be compared to some categories here, (i.e. category 11 and category 14).  The charts below show how efficiently we could calculate the percent, to determine if it is more or less than 80%, and answer the question.

\newpage
In any case, comparing performance those are our parameters... the size of $Q$, size of $P$, and the distance $d$.  We chart the results here, for both the NAIVE-NA and TRILAT-NA queries:

```{r QP_size_comparison_d01, echo=FALSE, message=FALSE, fig.cap="SQL Timings By Category (s) for distance d=0.1 - lower is better.  Each square represents a query with a given number of search points P (x-axis) and query points Q (y-axis) with fixed adequacy distance d.  The numbers represent the time (in seconds) to determine the NAP: percent of points in Q within distance d of any point in P.", fig.width=10, fig.height=4}
library(ggplot2)
library(data.table)
library('tidyverse')


data <- read.csv('../postgresql/query_timings.csv')
names(data)[3] <- 'timing_s'

data$dist <- 0
data[data$notes %like% 'dist: 0.1','dist'] <- 0.1
data[data$notes %like% 'dist: 1','dist'] <- 1
data[data$notes %like% 'dist: 10','dist'] <- 10
data[data$notes %like% 'dist: 100','dist'] <- 100

data <- data[data$notes %like% 'Adequ',]
data$algorithm = 'None'
data[data$notes %like% 'CatAdequacy','algorithm'] = 'st_distance'
data[data$notes %like% 'CatTrilatAdequacy', 'algorithm'] = 'Trilateration'
data <- data[data$algorithm != 'None',]
data$dist <- as.factor(data$dist)


data$leftcat <- as.factor(data$leftcat)
data$rightcat <- as.factor(data$rightcat)

data$leftcount <- as.factor(data$leftcount)
data$rightcount <- as.factor(data$rightcount)

plotdata <- aggregate(data$timing_s,
          by = list(data$leftcount, data$rightcount, data$algorithm, data$dist),
          FUN=median)
names(plotdata) <- c('leftcount', 'rightcount', 'algorithm', 'dist', 'timing_s')

library(gridExtra)

mydist=0.1

  p1 <- ggplot(data=plotdata[plotdata$algorithm=='st_distance' & plotdata$dist==mydist,], aes(x=leftcount, y=rightcount)) +
    geom_tile(aes(fill=timing_s)) +
    geom_text(aes(label=trunc(timing_s)), colour="black") +
              # colour="green") +
    # scale_fill_continuous(limits=c(0,1000))
    # scale_fill_gradient(low="black", high="white", limits=c(0,1000)) +
    xlab("|P| - Naive") +
    ylab("|Q|") +
    scale_fill_gradient2(
      low = "red",
      mid = "white",
      high = "blue",
      midpoint = 300, limits=c(0, 1000),
      space = "Lab",
      na.value = "grey50",
      guide = "colourbar",
      aesthetics = "fill"
    )
  
  p2 <-  ggplot(data=plotdata[plotdata$algorithm=='Trilateration' & plotdata$dist==mydist,], aes(x=leftcount, y=rightcount)) +
    geom_tile(aes(fill=timing_s)) +
    geom_text(aes(label=trunc(timing_s)),
              colour="black") +
    xlab("|P| - Trilateration") +
    ylab("|Q|") +
    # scale_fill_gradient(low = "blue", high = "red")
    # scale_fill_continuous(type='viridis', limits=c(0,1000)) +
    # scale_color_gradient(low='green', high='black', limits=c(0, 1000)) +
    # scale_fill_gradient(low="black", high="white", limits=c(0,1000)) +
    scale_fill_gradient2(
      low = "red",
      mid = "white",
      high = "blue",
      midpoint = 300, limits=c(0, 1000),
      space = "Lab",
      na.value = "grey50",
      guide = "colourbar",
      aesthetics = "fill"
    )
    # scale_fill_gradient2(midpoint = 100, low = "blue", mid = "white", high = "red", space = "Lab" )
    # facet_wrap(~notes)

  grid.arrange(p1, p2, nrow=1, ncol=2)

# boxp <- ggplot(data=plotdata[plotdata$dist==mydist,], aes(x=algorithm, y=timing_s, fill=algorithm)) +
#   geom_boxplot(outlier.colour="black", outlier.shape=16,
#              outlier.size=2, notch=TRUE, width=0.25, alpha=0.5) +
#   ggdist::stat_halfeye(adjust = 0.2, justification = -.2,
#     .width = 0, point_color = NA
#   ) +
#   xlab('Timing in seconds') +
#   scale_y_continuous(trans='log10') +
#   coord_flip()
# 
#   grid.arrange(boxp, p1, p2, widths = c(1,1,1),
#                layout_matrix = rbind(c(1,1), c(2,3)))

```

```{r QP_size_comparison_d1, echo=FALSE, message=FALSE, fig.cap="SQL Timings By Category (s) for distance d=1 - lower is better.", fig.width=10, fig.height=4}
library(ggplot2)
library(data.table)

data <- read.csv('../postgresql/query_timings.csv')
names(data)[3] <- 'timing_s'

data$dist <- 0
data[data$notes %like% 'dist: 0.1','dist'] <- 0.1
data[data$notes %like% 'dist: 1','dist'] <- 1
data[data$notes %like% 'dist: 10','dist'] <- 10
data[data$notes %like% 'dist: 100','dist'] <- 100

data <- data[data$notes %like% 'Adequ',]
data$algorithm = 'None'
data[data$notes %like% 'CatAdequacy','algorithm'] = 'st_distance'
data[data$notes %like% 'CatTrilatAdequacy', 'algorithm'] = 'Trilateration'
data <- data[data$algorithm != 'None',]
data$dist <- as.factor(data$dist)


library('tidyverse')
data$leftcat <- as.factor(data$leftcat)
data$rightcat <- as.factor(data$rightcat)

data$leftcount <- as.factor(data$leftcount)
data$rightcount <- as.factor(data$rightcount)

plotdata <- aggregate(data$timing_s,
          by = list(data$leftcount, data$rightcount, data$algorithm, data$dist),
          FUN=median)
names(plotdata) <- c('leftcount', 'rightcount', 'algorithm', 'dist', 'timing_s')

library(gridExtra)

mydist = 1
  p1 <- ggplot(data=plotdata[plotdata$algorithm=='st_distance' & plotdata$dist==mydist,], aes(x=leftcount, y=rightcount)) +
    geom_tile(aes(fill=timing_s)) +
    geom_text(aes(label=trunc(timing_s)), colour="black") +
              # colour="green") +
    # scale_fill_continuous(limits=c(0,1000))
    # scale_fill_gradient(low="black", high="white", limits=c(0,1000)) +
    xlab("|P| - Naive") +
    ylab("|Q|") +
    scale_fill_gradient2(
      low = "red",
      mid = "white",
      high = "blue",
      midpoint = 100, limits=c(0, 500),
      space = "Lab",
      na.value = "grey50",
      guide = "colourbar",
      aesthetics = "fill"
    )
  
  p2 <-  ggplot(data=plotdata[plotdata$algorithm=='Trilateration' & plotdata$dist==mydist,], aes(x=leftcount, y=rightcount)) +
    geom_tile(aes(fill=timing_s)) +
    geom_text(aes(label=trunc(timing_s)),
              colour="black") +
    xlab("|P| - Trilateration") +
    ylab("|Q|") +
    # scale_fill_gradient(low = "blue", high = "red")
    # scale_fill_continuous(type='viridis', limits=c(0,1000)) +
    # scale_color_gradient(low='green', high='black', limits=c(0, 1000)) +
    # scale_fill_gradient(low="black", high="white", limits=c(0,1000)) +
    scale_fill_gradient2(
      low = "red",
      mid = "white",
      high = "blue",
      midpoint = 100, limits=c(0, 500),
      space = "Lab",
      na.value = "grey50",
      guide = "colourbar",
      aesthetics = "fill"
    )
    # scale_fill_gradient2(midpoint = 100, low = "blue", mid = "white", high = "red", space = "Lab" )
    # facet_wrap(~notes)
  
  grid.arrange(p1, p2, nrow=1, ncol=2)

```

```{r QP_size_comparison_d10, echo=FALSE, message=FALSE, fig.cap="SQL Timings By Category (s) for distance d=10 - lower is better. Note how query times go up quickly as the query point size (y-axis) increases, but shows a decrease then increase as the search point cardinality (x-axis) increases.", fig.width=10, fig.height=4}
library(ggplot2)
library(data.table)

data <- read.csv('../postgresql/query_timings.csv')
names(data)[3] <- 'timing_s'

data$dist <- 0
data[data$notes %like% 'dist: 0.1','dist'] <- 0.1
data[data$notes %like% 'dist: 1','dist'] <- 1
data[data$notes %like% 'dist: 10','dist'] <- 10
data[data$notes %like% 'dist: 100','dist'] <- 100

data <- data[data$notes %like% 'Adequ',]
data$algorithm = 'None'
data[data$notes %like% 'CatAdequacy','algorithm'] = 'st_distance'
data[data$notes %like% 'CatTrilatAdequacy', 'algorithm'] = 'Trilateration'
data <- data[data$algorithm != 'None',]
data$dist <- as.factor(data$dist)


library('tidyverse')
data$leftcat <- as.factor(data$leftcat)
data$rightcat <- as.factor(data$rightcat)

data$leftcount <- as.factor(data$leftcount)
data$rightcount <- as.factor(data$rightcount)

plotdata <- aggregate(data$timing_s,
          by = list(data$leftcount, data$rightcount, data$algorithm, data$dist),
          FUN=median)
names(plotdata) <- c('leftcount', 'rightcount', 'algorithm', 'dist', 'timing_s')

library(gridExtra)

mydist = 10
  p1 <- ggplot(data=plotdata[plotdata$algorithm=='st_distance' & plotdata$dist==mydist,], aes(x=leftcount, y=rightcount)) +
    geom_tile(aes(fill=timing_s)) +
    geom_text(aes(label=trunc(timing_s)), colour="black") +
              # colour="green") +
    # scale_fill_continuous(limits=c(0,1000))
    # scale_fill_gradient(low="black", high="white", limits=c(0,1000)) +
    xlab("|P| - Naive") +
    ylab("|Q|") +
    scale_fill_gradient2(
      low = "red",
      mid = "white",
      high = "blue",
      midpoint = 20, limits=c(0, 100),
      space = "Lab",
      na.value = "grey50",
      guide = "colourbar",
      aesthetics = "fill"
    )
  
  p2 <-  ggplot(data=plotdata[plotdata$algorithm=='Trilateration' & plotdata$dist==mydist,], aes(x=leftcount, y=rightcount)) +
    geom_tile(aes(fill=timing_s)) +
    geom_text(aes(label=trunc(timing_s)),
              colour="black") +
    xlab("|P| - Trilateration") +
    ylab("|Q|") +
    # scale_fill_gradient(low = "blue", high = "red")
    # scale_fill_continuous(type='viridis', limits=c(0,1000)) +
    # scale_color_gradient(low='green', high='black', limits=c(0, 1000)) +
    # scale_fill_gradient(low="black", high="white", limits=c(0,1000)) +
    scale_fill_gradient2(
      low = "red",
      mid = "white",
      high = "blue",
      midpoint = 20, limits=c(0, 100),
      space = "Lab",
      na.value = "grey50",
      guide = "colourbar",
      aesthetics = "fill"
    )
    # scale_fill_gradient2(midpoint = 100, low = "blue", mid = "white", high = "red", space = "Lab" )
    # facet_wrap(~notes)
  
  grid.arrange(p1, p2, nrow=1, ncol=2)

```

```{r QP_size_comparison_d100, echo=FALSE, message=FALSE, fig.cap="SQL Timings By Category (s) for distance d=100 - lower is better", fig.width=10, fig.height=4}
library(ggplot2)
library(data.table)

data <- read.csv('../postgresql/query_timings.csv')
names(data)[3] <- 'timing_s'

data$dist <- 0
data[data$notes %like% 'dist: 0.1','dist'] <- 0.1
data[data$notes %like% 'dist: 1','dist'] <- 1
data[data$notes %like% 'dist: 10','dist'] <- 10
data[data$notes %like% 'dist: 100','dist'] <- 100

data <- data[data$notes %like% 'Adequ',]
data$algorithm = 'None'
data[data$notes %like% 'CatAdequacy','algorithm'] = 'st_distance'
data[data$notes %like% 'CatTrilatAdequacy', 'algorithm'] = 'Trilateration'
data <- data[data$algorithm != 'None',]
data$dist <- as.factor(data$dist)


library('tidyverse')
data$leftcat <- as.factor(data$leftcat)
data$rightcat <- as.factor(data$rightcat)

data$leftcount <- as.factor(data$leftcount)
data$rightcount <- as.factor(data$rightcount)

plotdata <- aggregate(data$timing_s,
          by = list(data$leftcount, data$rightcount, data$algorithm, data$dist),
          FUN=median)
names(plotdata) <- c('leftcount', 'rightcount', 'algorithm', 'dist', 'timing_s')

library(gridExtra)

mydist = 100
  p1 <- ggplot(data=plotdata[plotdata$algorithm=='st_distance' & plotdata$dist==mydist,], aes(x=leftcount, y=rightcount)) +
    geom_tile(aes(fill=timing_s)) +
    geom_text(aes(label=trunc(timing_s)), colour="black") +
              # colour="green") +
    # scale_fill_continuous(limits=c(0,1000))
    # scale_fill_gradient(low="black", high="white", limits=c(0,1000)) +
    xlab("|P| - Naive") +
    ylab("|Q|") +
    scale_fill_gradient2(
      low = "red",
      mid = "white",
      high = "blue",
      midpoint = 20, limits=c(0, 60),
      space = "Lab",
      na.value = "grey50",
      guide = "colourbar",
      aesthetics = "fill"
    )
  
  p2 <-  ggplot(data=plotdata[plotdata$algorithm=='Trilateration' & plotdata$dist==mydist,], aes(x=leftcount, y=rightcount)) +
    geom_tile(aes(fill=timing_s)) +
    geom_text(aes(label=trunc(timing_s)),
              colour="black") +
    xlab("|P| - Trilateration") +
    ylab("|Q|") +
    # scale_fill_gradient(low = "blue", high = "red")
    # scale_fill_continuous(type='viridis', limits=c(0,1000)) +
    # scale_color_gradient(low='green', high='black', limits=c(0, 1000)) +
    # scale_fill_gradient(low="black", high="white", limits=c(0,1000)) +
    scale_fill_gradient2(
      low = "red",
      mid = "white",
      high = "blue",
      midpoint = 20, limits=c(0, 60),
      space = "Lab",
      na.value = "grey50",
      guide = "colourbar",
      aesthetics = "fill"
    )
    # scale_fill_gradient2(midpoint = 100, low = "blue", mid = "white", high = "red", space = "Lab" )
    # facet_wrap(~notes)
  
  grid.arrange(p1, p2, nrow=1, ncol=2)

```

***

### Interpretation

We find these charts quite interesting.  What is happening?  we make several distinct observations:

#### Accuracy
First we wanted to reaffirm that the code remained 100% accurate... both algorithms agreed with one another completely, and the NAP and adequacy thresholds were calculated identically.  We focus on performance in our results, then, given the 100% accuracy of all approaches, as expected.

#### Processing time is directly realted to |Q|
As the size of the query data set ($Q$) increases along the y-axis, it's clear that the computation time increases at roughly the same rate.  The top row is roughly 3x the row below it (the ratio of 32,000:10,000)... the next row is roughly half (10,000:5000), and so on.  This makes sense; if we consider the query a for loop over $Q$, which is how we implement it in another language, the cost should go up proportionately, and it clearly does.

#### The diagonal is strangely fast.

This we originally thought of as a flaw in our experimental design, but now that we analyze it it's quite interesting and we chose not to change it.  The situation along the diagonal is that we're using the same category for $P$ and $Q$; this means that the data sets are identical, and that there is a 100% chance, always, of finding a point $p$ within $d$ of $q \forall q \in Q$ namely the point itself - since it is in the data set and we made no effort to restrict this.  There may or may not be a second point within distance, but recall that our definition only requires one (since we did not extend to $k-NA$).  But why is this so fast?  Given the guaranteed success of finding at least one qualifying point in every data set, we surmise that the performance improvement is a statistical result of almost never having to iterate over the entire set $P$.  The average time to find a point if one is guaranteed to exist, should be roughly half the time (over repeated iterations) of searching the whole set.  The entire query time is then some statistical fraction of a prior expectation of the likelihood of finding a match.

Given our chart of $d$ vs. $NAP$ earlier, it makes sense that the diagonal's relative value is higher when $d$ is low.  That is, when $d=100$ every point is adequate, even without itself as a guaranteed qualifying point, so there is no real performance gain -- we never have to search every $p$ for a match.  When $d=0.1$, on the other hand, we have to search the entire data set, in vain, over 50% of the time (per our chart), wasting an entire scan of $P$.  Along the diagonal this never happens, and the time results mirror this fact.

#### The impact of varying distance d

Our best results are on low $d$ with high $|Q|$ and high $|P|$.  The NAIVE-NA query took nearly 15 minutes on our hardware to execute a 32,000x32,000 point search for network adequacy for $d=0.1$.  A little over 4 minutes for $d=1$, and just over and under a minute for $d=10$ and $d=100$ respectively.  A similar pattern exists for the $TRILAT-NA$ algorithm.

The reason for this seems straightforward... the smaller the distance $d$, the less likely it is that any given point $p$ is within $d$ of a query point $q$.  Thus, in a large sample $P$ we expect to have to search more points, and spend more time, as $d$ decreases.  This is a simple theory consistent with the result.

#### Complex relationship with |P| for NAIVE-NA

A more complicated relationship exists as we walk the charts from left to right.  Examine the chart where $d=1$ for a moment.  On every but the bottom line (where zeroes probably hide this as a rounding problem), on the NAIVE-NA chart, the query time is HIGHER for $|P|=100$ than it is for $|P| \in (900, 2000, 5000, 10000)$, however it picks UP again when $|P|=32000$.

Why?  We've run these tests repeatedly, in random order, back to back, to rule out hot/cold cache issues or other start-up problems.

Let's look at the actual NAP values for that row:

\singlespacing

```{r d_1_NAP_chart, echo=FALSE}
library(data.table)
library(knitr)
dnap <- data.table(P=c(100, 900, 2000, 5000, 10000, 32000),
                   NAP=c('36.27%', '70.81%', '76.37%', '83.66%', '100.00%', '93.50%'))
kable(dnap, caption="P vs. ~NAP for d=0.1, Q=10000", digits=3)
```

\doublespacing

With only 100 candidate points, only 36.27% of $Q$ is within 0.1 miles of a point in $P$.  The other 63.73% must iterate all $|P|*|Q|$ comparisons (in the NAIVE-NA implementation), resulting in $0.6473*10000*100=647,300$ comparisons plus a portion of the other 36.27% which is at most $0.3627*10000*100=362,700$.  Applying the same math for $|P|=900$, at 70.81% (29.19% inadequate) we get $0.2919*10000*900=2,627,100$.  So, we see no direct theoretical observation that would make this make sense.

Sooo... we have to look at the SQL query plan:

\singlespacing

```{SQL explain_d_p900}
 Aggregate  (cost=37295851666.13..37295851666.14 rows=1 width=16)
      (actual time=86693.568..86693.570 rows=1 loops=1)
   ->  Nested Loop Left Join  (cost=0.00..37295851616.50 rows=9925 width=8)
          (actual time=152.251..86690.574 rows=10000 loops=1)
         ->  Seq Scan on sample_cat_ref_dists m
              (cost=0.00..4018.00 rows=9925 width=60)
              (actual time=151.943..166.146 rows=10000 loops=1)
               Filter: (category = 14)
               Rows Removed by Filter: 140000
         ->  Limit  (cost=0.00..3757768.00 rows=1 width=4)
                    (actual time=8.651..8.651 rows=0 loops=10000)
               ->  Seq Scan on sample_cat_ref_dists p
                    (cost=0.00..3757768.00 rows=1 width=4)
                    (actual time=8.649..8.649 rows=0 loops=10000)
                     Filter: category=10 ...
                     Rows Removed by Filter: 95609
 Planning Time: 0.464 ms
 JIT:
   Functions: 11
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 4.194 ms
   Inlining 9.301 ms
   Optimization 86.339 ms
   Emission 55.402 ms
   Total 155.236 ms
 Execution Time: 86697.917 ms
(15 rows)

Time: 86699.517 ms (01:26.700)
```


```{SQL explain_d_p100}
Aggregate  (cost=3107991549.46..3107991549.47 rows=1 width=16)
           (actual time=43705.837..43705.839 rows=1 loops=1)
   ->  Nested Loop Left Join  (cost=0.00..3107991499.83 rows=9925 width=8)
          (actual time=158.346..43702.035 rows=10000 loops=1)
         ->  Seq Scan on sample_cat_ref_dists m
              (cost=0.00..4018.00 rows=9925 width=60)
              (actual time=158.024..172.995 rows=10000 loops=1)
               Filter: (category = 14)
               Rows Removed by Filter: 140000
         ->  Limit  (cost=0.00..313147.33 rows=1 width=4)
                    (actual time=4.351..4.351 rows=1 loops=10000)
               ->  Seq Scan on sample_cat_ref_dists p
                   (cost=0.00..3757768.00 rows=12 width=4)
                   (actual time=4.349..4.349 rows=1 loops=10000)
                     Filter: category=11 ...
                     Rows Removed by Filter: 43992
 Planning Time: 0.189 ms
 JIT:
   Functions: 11
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 1.702 ms
   Inlining 9.062 ms
   Optimization 91.460 ms
   Emission 56.565 ms
   Total 158.790 ms
 Execution Time: 43707.634 ms
(15 rows)

Time: 43708.659 ms (00:43.709)
```

\doublespacing

There are two key differences here - the inner (second) "Rows Removed by Filter" which, for $|P|=100$ is $95609$ and for $|P|=900$ is $43992$.  The other, related, difference being the actual time in the loops - $86693.568$ for the outer and $8.649$ for the inner when $|P|=100$ and $43705.837$ outer, $4.349$ inner for $|P|=900$.  This indicates that our analysis was flawed; the SQL optimizer swapped what we presumed were the inner and outer loops, and given the percents, the inner loop is more expensive when $|P|=100$ (by about double).


#### Complex relationship with |P| for TRILAT-NA

The pattern is similar, but not identical, in the right half of the graphs - those using our new TRILAT-NA algorithm.

If we examine the top rows for $d=0.1$ and $d=1$, where runtime is longest, the NAIVE-NA algorithm as we described may slightly increase performance for low $|P|$, but quickly sees a runtime increase as $|P|$ increases.  The TRILAT-NA algorithm, however, while it isn't monotonic for $|P| \in (100, 900)$, it actually _decreases_ as $|P|$ increases from then on (with the exception of items on the diagonal, which are special as we discussed before).

This is easier to explain... the addition of the filters using the $refdist_i$ distances causes a much quicker elimination of points outside of the $d$ radius from each $q$.  More points in $P$ makes the efficiency of these culls more pronounced.  That is, if we look at the "Rows Removed by Filter" as we did in the explain before, and remember the figure [Monte Carlo Estimating Ring Overlap Area], we see that those three additional $refdist$ filters will exclude a high percent of candidate records quickly.  The _remaining_ points, subject to the expensive st_distance calculation, have a high percent chance of being within $d$ of $q$.

The higher the number of points in $P$, the more points culled as a result of the $refdist$ filters that no longer have to have st_distance called, compared to the NAIVE-NA query, resulting in the performance gains we see.

#### NA Performance Conclusions

The TRILAT-NA algorithm was as fast or faster in almost every test we ran; the only major outlier was when $d=0.1$ and $|P|=100$, in two cases ($|Q|=2000$ and $|Q|=10000$), TRILAT-NA was slower by 2 and 14 seconds (about 7% and 10%) respectively.  When $d=1$, $|P|=100$ again, and $|Q|=900$, $|Q|=5000$, or $|Q|=10000$, TRILAT-NA was slower by 1 second or less in each case.  Given that our resolution is 1-second, these are statistically minor.  Even for the larger discrepancies, it seems that the TRILAT-NA algorithm is only slower for very small values of $|P|$, which is where optimization is least needed in the first place.  When the workload takes the longest, the TRILAT-NA algorithm performs significantly faster (up to 100 times, per these charts) than the naive algorithm.
