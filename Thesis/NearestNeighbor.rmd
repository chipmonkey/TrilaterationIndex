## Multilateration NN Algorithms

We present a new [Trilateration (TRI)] algorithm, which uses the Trilateration Index to solve the Nearest Neighbor problem.

We include three other algorithms derived from the original TRI algorithm, but with various algorithmic changes which we describe in depth below.  These were the result of speculation on our part for potential improvements, our [Experimentation] section will describe how these perform in various dimensions and metric spaces.  In particular we describe three variations on our TRI algorithm: [TrilaterationApprox (TIA)], [TrilaterationExpand (TIE)], and [TrilaterationExpand2 (TIE2)].  These are based on simple core well established algorithms - sorted list searches, heaps, and the base index algorithms we described in [Simple Multilateration Index Operations].

### Comparing Algorithms

Recall from our [Review of Literature], we are comparing $NN$ algorithms by four areas Training Time Complexity, Memory Space, Prediction Time Complexity, Insertion/Move Complexity.

For each of these Trilateration algorithms, complexity is:

1. Training Time Complexity is $O(|P|)$ - each point $p \in P$ is compared to the $d+1$ reference points ($d + 1 = 3$ for Trilateration).
2. Memory Space is $|P|*(d+1)$ since each structure requires the storage of a $d*p$ array of $d+1$ distances for each point $p \in P$
3. Prediction Time Complexity is, as with other $kNN$ algorithms, bounded by worst-case of $O(n)$.  The algorithms may differ in their average case performance.
4. Insertion Complexity is $O(|P|)$ - being the cost if inserting or updating an element in the sorted array of distances for each $p \in P$.

For comparison, we assign a somewhat arbitrary numeric scale to the $O()$ complexity values researched in our earlier review.  This helps exemplify how we sacrifice worst-case prediction complexity, but gain some average prediction time, in exchange for Training and Insertion performance improvements.

```{r o_comparison_table, echo=FALSE}
library(knitr)
k = data.frame(Algorithm=c("Brute-Force", "Space Partitioning Trees",
                           "Locality Sensitive Hashing", "Graph Based Search",
                           "Multilateration"),
               TrainingTime = c(0, 2, 2, 5, 1),
               MemorySpace = c(1, 1, 3, 3, 1),
               WorstPredictionTime = c(5, 2, 3, 1, 5),
               AveragePredictionTime = c(5, 2, 2, 1, 3),
               InsertionTime = c(1, 2, 3, 5, 2))

kable(k, caption="Approximate relative complexity on a 1-5 scale")
```

We explore four variations to explore various possible optimizations that we imagined prior to experimentation, since the experiment was required to determine which would perform best under which circumstances.  While we go into more detail later, in short:
* TRI - search along a sorted 1-dimensional ($d_1) distance until the worst 1d distance is farther than the top-k nearest so far
* TIA - an approximate solution based solely on a 1-dimensional distance (this turns out not to be very good)
* TIE - set a low and high bound on a radius (r) using the efficient $Within()$ function to bisect the range until $~k$ records remain
* TIE2 - start with a radius $r$ being the distance of the closest point along 1-dimension and select the next multiple of $k$ (a "chunk") points along that dimension at a time until $k$ nearest records are identified


#### Trilateration (TRI)

The main trilateration algorithm for exact Nearest-Neighbor solutions takes the Trilateration Index (recall - the distances stored in a sorted array form from all points in $P$ with respect to $d+1$ fixed reference points -- 3 in the case of 2-d Trilateration) and applies the following for a query point $q$.  This provides simple a simple $O(log(n))$ binary search by distance, along with the ability to quickly iterate through points consecutively closer or farther from a given point in list order using common array operations.[@Tainiter1963]

```{r trilateration_nn_psudocode, eval=FALSE, attr.source='.numberLines'}
Calculate qd1..qdn as the distances from point q to the n reference points r1..rn
Find the index i1 in TI for the nearest point along the d1 distance to q
Create HEAP - a max heap of size k
    Let WORST_DIST bet the maximum distance on HEAP at any time
Calculate LOW_IDX = i1-(k/2) and HIGH_IDX = i1+(k/2)
For all points c in TI between TI[LOW_IDX] and TI[HIGH_IDX]:
    push c onto HEAP
Find the index LOW_IDX_POSSIBLE in TI as:
    the highest point along d1 where |TI[,d1]-qd1| > WORST_DIST
Find the index HIGH_IDX_POSSIBLE in TI as:
    the lowest point along d1 where |TI[,d1]-qd1| > WORST_DIST
While LOW_IDX > LOW_IDX_POSSIBLE or HIGH_IDX < HIGH_IDX_POSSIBLE:
    Choose the closer of TI[LOW_IDX-1, d1] or TI[HIGH_IDX+1, d1] along d1 (call it c)
        If ALL of |TI[c, dx]-qx| (for all x 2..n) are < WORST_DIST
          Calculate D(q, c)
          If D(q, c) < WORST_DIST:
              Add c to the HEAP
              recalculate LOW_IDX_POSSIBLE and HIGH_IDX_POSSIBLE
        depending on the choice, decrement LOW_IDX or increment HIGH_IDX
Return HEAP
```

Basically, looking only along one distance dimension (proximity to d1), find the closest k points to q (which is very quick along a sorted array - O(log n) to find the first point and O(1) to add and subtract k/2 to the indices to get the boundary).  Expand the low and high values (selecting the next point as the closest of the 2 along d1) until we have k points such that the farthest (worst) distance to one of those points is closer than the distance along d1 for any other point (which is bounded by LOW_IDX_POSSIBLE and HIGH_IDX_POSSIBLE, since that distance is our sort order).

#### TrilaterationApprox (TIA)

In an attempt to gain benefit from the relaxed constraints of an "approximate" $aNN$ approach, we experiment with an algorithm that effectively excludes the distance calculations altogether, from our TRI algorithm altogether.  Recall that, for a given $d_x$ distance to reference point $r_x$, a point $p$ can be _no closer than_ $q_{dx} - p_{dx}$; if we treat the approx_distance $(q, p) = \frac{\sum_{x=1}^{m}{q_{dx} - p_{dx}}}{|d|}$ (the mean of the relative distances from $q$ to all reference points $r_x$), or, more aggressively, the minimum such distance, we can return an approximate result without ever having to call the distance function itself (after the index is created).

We ended up abandoning this approach after only a few tests - there was a significant drop in recall (the mechanism by which ann-benchmarks measures effectiveness of $aNN$ algorithms), with no particular improvement in performance.  This effectively removed $aNN$ from our consideration; our results are primarily focused on exact $NN$ results as a consequence.

#### TrilaterationExpand (TIE)

We theorized that the TRI approach may be slowed down by the overhead of having to iterate one point at a time, and by not utilizing more than one reference point early in the process.  Given the efficiency of the $Within()$ function (see [Time Complexity]), we wondered if we should treat the distance $d$ as the target variable, and use an incremental search to zero in on the proper value to result in $k$ neighbors within $d$.

This turns out to be a silly idea, once we get the results back, but here we are.  The algorithm would look like:

```{r trilateration_tie_psudocode, eval=FALSE, attr.source='.numberLines'}
set radius = 0.5
set too_low = 0
set too_high = maximum possible distance in the space
set x = CountWithin(radius, q, P)
while x != k:
    if x < k:
        set too_low = radius
        set radius = (radius + too_high)/2
    else:
        set too_high = radius
        set radius = (radius + too_low)/2
return Within(radius, q, P)
```


#### TrilaterationExpand2 (TIE2)

Another approach to minimizing the overhead of expanding the range in the TRI algorithm by one at a time is to simply expand by some fixed amount $> 1$.  This actually shows performance gains when the distance functions are inexpensive, although not enough to really be competitive with other $NN$ solutions, but shows no benefit (in fact, it incurs quite the cost) when using our expensive geodesic distance functions.  See our results section for more details.

Fundamentally, the change to the TRI algoritm is that we expand by $k$ (which is a convenient constant), or some larger constant, at a time, rather than $1$ point along $d_1$ in $TI$.  In effect:

```{r trilateration_tie2_psudocode, eval=FALSE, attr.source='.numberLines'}
Set CHUNK equal to the greater of k or 500
Calculate qd1..qdn as the distances from point q to the n reference points r1..rn
Find the index i1 in TI for the nearest point along the d1 distance to q
Create HEAP - a max heap of size k
    Let WORST_DIST bet the maximum distance on HEAP at any time
Calculate LOW_IDX = i1-(k/2) and HIGH_IDX = i1+(k/2)
For all points c in TI between TI[LOW_IDX] and TI[HIGH_IDX]:
    push c onto HEAP
Find the index LOW_IDX_POSSIBLE in TI as:
    the highest point along d1 where |TI[,d1]-qd1| > WORST_DIST
Find the index HIGH_IDX_POSSIBLE in TI as:
    the lowest point along d1 where |TI[,d1]-qd1| > WORST_DIST
While LOW_IDX > LOW_IDX_POSSIBLE or HIGH_IDX < HIGH_IDX_POSSIBLE:
    If TI[LOW_IDX-1, d1] is closer than TI[HIGH_IDX+1, d1]:
        PRIOR_IDX = LOW_IDX
        LOW_IDX = LOW_IDX - CHUNK
        Evaluate points between TI[LOW_IDX,] and TI[PRIOR_IDX,]:
          If ALL of |TI[c, dx]-qx| (for all x 2..n) are < WORST_DIST
          Calculate D(q, c)
          If D(q, c) < WORST_DIST:
              Add c to the HEAP
    else:
        PRIOR_IDX = HIGH_IDX
        HIGH_IDX = HIGH_IDX + CHUNK
        Evaluate points c between TI[PRIOR_IDX,] and TI[HIGH_IDX,]:
          If ALL of |TI[c, dx]-qx| (for all x 2..n) are < WORST_DIST
          Calculate D(q, c)
          If D(q, c) < WORST_DIST:
              Add c to the HEAP
    recalculate LOW_IDX_POSSIBLE and HIGH_IDX_POSSIBLE
Return HEAP
```

#### Implementation Notes

There's a lot to digest in these algorithms; many choices were made, and various performance issues were encountered.  Of note, many of the implementation specifics were due to building our algorithms on top of the existing scikit-learn code.[@scikit-learn]  The use of the $HEAP$ structure, mentioned in our code, is immediately attributable to leveraging scikit-learn's source.  Also, being built in Cython, and having been field-tested for about 10 years, it's possible our code could be improved or may have bugs compared to the rest of scikit-learn, but we tried real hard.

As mentioned before, the TIA algorithm failed dominantly because either performance or recall were too slow... recall was a problem when only 1 reference point was used, and performance faltered with multiple reference points due to the high cost of calculating the intersection of candidate point lists from multiple reference points.

The implementation on both "Expanding" algorithms (TIE and TIE2) presented many choices.  Initially we sought to find a reasonable initial guess for the candidate $radius$, however no suitable algorithm presented itslef that was superior to guessing "0.5" as a first guess (curiously true regardless of the coordinate scale).  Similarly, the value of 500 for $CHUNK$ size was found reasonable via trial and error, although heuristics to arrive at the number, rather than hard-coding it, could probably benefit specific cases.

See [Experimental Results] and [Experimentation] for the results and specifics of how we tested these algorithms.