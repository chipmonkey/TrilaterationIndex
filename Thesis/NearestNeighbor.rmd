## Nearest Neighbor

The nearest neighbor ($NN$) problem should need no introduction.  For our perspective, we talk about NN and k-NN as:

Given a non-empty set of points $P$, a non-empty set of query points $Q$ in a metric space $M$, and a distance function $D(a, b)$ describing the distance between points $a$ and $b$ for $a \in M$ and $b \in M$, the "Nearest Neighbor" of a given point $q \in Q$ is the point $p \in P$ such that $D(p, q)$ is the lowest value of $D(p', q)$ over all points $p' \in P$ (i.e. $D(p, q) < D(p',q) \forall p'$ with $p \ne p'$.

Note that it is possible that such a point does not exist if there are multiple points with the same lowest distance; we do not explore that situation here, as it does not affect our examination.

The k-nearest neighbors ($kNN$) of a given point $q$ as above is the list of $k$ points $R = p_1..p_k \in P$ such that $D(p_k, q)$ is the lowest value of $D(p, q)$ over all $p \in P$ such that $D(p', q)$ for $p' \ne p$ and $p' \not \in R$.  It should be evident that $NN = kNN$ when $k = 1$.

An approximate nearest neighbor ($ANN$) algorithm is one which will provide $k$ points $R' = p_1..p_k \in P$, however it does *_not_* guarantee that there exists no point in $P \not \in R'$ closer than any point in $R'$.  Some formulations require that, if there *_is_* such a point $p'$, that it cannot be more than some $\epsilon$ farther from $q$ than any point $p_i \in R'$.  In general, $ANN$s are used when we can get a 'close enough' solution algorithmically faster than a perfect $kNN$ solution.  For our purposes, we largely ignore $ANN$s except for their historical value, as our construction did not yield algorithms that exhibited this beneficial tradeoff.  See the "TrilatApprox" algorithm section for some more discussion.

### A History of k-NN Solving Algorithms

#### Brute-Force
The naive approach to solving k-nn is a brute-force algorithm, iterating over every point $p in P$ and keeping track of the lowest k distances.  This is trivially a $O(n)$ query time.


#### Space Partitioning Trees

Space paritioning trees use a trie to arrange points from $p$ into groups with a hierarchical search structure, such that, generally, points which are close to one another exist in nearby hierarchies.  The $k-d$ tree was described in 1975.[@Bentley1975]  This partitions a space by dividing the underlying points at the median point along one of the dimensional axes, recursively, resulting in a searchable trie.  An adaptation of this - the Ball-Tree - partitions the space into hyperspheres, rather than along dimensional axes.[@Liu2006]

These are straightforward structures that are easy to describe and implement.  


#### Locality Sensitive Hashing

LSH stands somewhat alone, as it is not literally space partitioning with strict divisions.  LSH relies on creating a hash function that hashes points into bins with a property that two points with the same hash have a high likelihood of being nearer to each other than points with different hash values.[@Indyk1998]

#### Graph Based Search

More recent algorithms, such as Facebook Research's FAISS, follow a graph based search structure.[@FAISS2017]

A good overview of this approach was available from Liudmila Prokhorenkova: "Recently, graph-based approaches were shown to demonstrate superior performance over other types of algorithms in many large-scale applications of NNS [@Amueller2020]. Most graph-based methods are based on constructing a nearest neighbor graph (or its approximation), where nodes correspond to the elements of D, and each node is connected to its nearest neighbors by directed edges.[@Dong2011] Then, for a given query q, one first takes an element in D (either random or fixed predefined) and makes greedy steps towards q on the graph: at each step, all neighbors of a current node are evaluated, and the one closest to q is chosen."[@Prokhorenkova2019]

The construction costs of these structures can be very high.  A Brute Force construction of a k-Nearest Neighbor Graph $(kNNG)$ has time complexity $O(n^2)$ which is of course completely untenable for large data sets.  Approaches exist to improve upon this, including improvements resulting in approximate results, but this class still tends to trade the highest construction cost for some of the fastest query times in high dimensions.[@Dong2011], [@Prokhorenkova2019]


### Comparing Algorithms
Training Time Complexity
Memory Space
Prediction Time Complexity
Insertion/Move Complexity

## Multilateration NN Algorithms

We create a total of four similar algorithms to exploit the Multilateration Index, in particular for geodesic distances (although our implementations are applicable to any distance function):  [Trilateration (TI)], [TrilaterationApprox (TIA)], [TrilaterationExpand (TIE)], and [TrilaterationExpand2 (TIE2)], as follows:

#### Trilateration (TI)

The main trilateration algorithm for exact Nearest-Neighbor solutions takes the Trilateration Index (recall - the distances stored in a sorted array form from all points in $P$ with respect to $d+1$ fixed reference points -- 3 in the case of 2-d Trilateration) and applies the following for a query point $q$:

```{x trilateration_nn_psudocode, eval=FALSE, attr.source='.numberLines'}
Calculate qd1..qdn as the distances from point q to the n reference points r1..rn
Find the index i1 in TI for the nearest point along the d1 distance to q
Calculate LOW_IDX = i1-(k/2) and HIGH_IDX = i1+(k/2)
Store all points in TI between TI[LOW_IDX] and TI[HIGH_IDX] in a max-heap by distance (HEAP)
    Let WORST_DIST bet the maximum distance on HEAP at any time
Find the index LOW_IDX_POSSIBLE in TI for highest point along d1 where |TI[,d1]-qd1| > WORST_DIST
Find the index HIGH_IDX_POSSIBLE in TI for the lowest point along d1 where |TI[,d1]-qd1| > WORST_DIST
While LOW_IDX > LOW_IDX_POSSIBLE or HIGH_IDX < HIGH_IDX_POSSIBLE:
    Choose the closer of TI[LOW_IDX-1, d1] or TI[HIGH_IDX+1, d1] along d1 (call it p')
        If ALL of |TI[p', dx]-qx| (for all x 2..n) are < WORST_DIST
          Calculate D(q, p')
          If D(q, p') < WORST_DIST:
              Add p' to the heap
              recalculate LOW_IDX_POSSIBLE and HIGH_IDX_POSSIBLE
        depending on the choice, decrement LOW_IDX or increment HIGH_IDX
Return HEAP
```

Basically, looking only along one distance dimension (proximity to d1), find the closest k points to q (which is very quick along a sorted array - O(log n) to find the first point and O(1) to add and subtract k/2 to the indices to get the boundary).  Expand the low and high values (selecting the next point as the closest of the 2 along d1) until we have k points such that the farthest (worst) distance to one of those points is closer than the distance along d1 for any other point (which is bounded by LOW_IDX_POSSIBLE and HIGH_IDX_POSSIBLE, since that distance is our sort order).

#### TrilaterationApprox (TIA)

#### TrilaterationExpand (TIE)

#### TrilaterationExpand2 (TIE2)