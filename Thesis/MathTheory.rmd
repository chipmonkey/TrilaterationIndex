---
title: "Math Theory"
author: "Chip Lynch"
date: "1/31/2021"
output: pdf_document
---


## Underlying Mathematical Concepts

The benefit of storing geographic points as a set of trilateration distances rather than latitude and longitude boils down to the simplification of comparing distances between points by shortcutting complex distance queries using simple subtractions.  We discuss the math behind the geospatial queries, to exhibit their complexity, and set some theoretical bounds on quick distance calculations using the trilateration index.


### High Cost of Geospatial Calculations

Calculating the distance between two points around the globe with precision is required for Satellite Communications and Geospatial Positioning Systems (GPS), as well as for ground based surveying and generally all applications requiring precise (sub-meter) measurements accounting for the curvature of the earth.[@ASPRS2015]

#### Haversine
One of the simplest distance calculations between two points on the earth's surface -- namely the Haversine Formula [@gade2010], which dates to the early 1800s-- works by assuming the earth is a sphere.  The calculation for the distance between two points on the earth, using this formula goes as:

Given the radius of a spherical representation of the earth as $r = 6356.752 km$ and the coordinates of two points (latitude, longitude) given by $(\phi_1, \lambda_1)$ and $(\phi_2, \lambda_2)$, the distance $d$ between those points along the surface of the earth is:

$$d = 2r\sin^{-1}(\sqrt{\sin^2(\frac{\phi_2-\phi_1}{2}) +\cos(\phi_1)\cos(\phi_2)\sin^2(\frac{\lambda_2-\lambda_1}{2})})$$

Obviously this is somewhat computationally complex, comprising five trigonometric functions, two subtractions and a square root.  While it is a closed form solution, it causes an error over long distances of up to 0.3%, which can mean distances are off by up to 3 meters over distances of 1000 kilometers.  From the equator to the north pole, which on a sphere is defined as precisely 10,000 km, the actual distance is off by over 2 km, which is a sizeable error for even the most robust applications.


#### Vincenty and Karney's Improvements (Geodesics)

The shortcomings of the spherical calculation was thoroughly discussed by Walter Lambert in 1942.[@Lambert1942]  However it wasn't until 1975 that an iterative computational approach came about to give more accurate distance measurements with a model of the earth more consistent with reality.  By considering the earth as an ellipsoid, rather than a sphere, the distance calculations are more complex, but far more precise.  Vincenty was able to create an iterative approach accurate down to the millimeter level on an ideal elliptical earth; far more accurate than the Haversine calculations[@Vincenty1975].  This algorithm, however, was a series which failed to converge for points at near opposite sides of the earth.  Karney was able to improve upon this in 2013 to fix these antipodal non-convergences, and the resulting formulae are now widely available in geospatial software libraries where precision is required (commonly referred to as "Geodesic" distances. [@Karney2013]

To get an idea of the relative complexity, we ran some basic timings using widely available python libraries that perform both calculations.  The Haversine is about 22 times faster than Karney's iterative approach.  For comparison, we include Euclidean functions, which are of course computationally simple, although their usefulness on curved surfaces are minimal:

```{r geodist_timings, echo=FALSE, results ='asis'}
library(knitr)
times <- data.frame(title=c("Geodesic", "Haversine", "Euclidean"),
                    time=c(1.211069, 0.0553729, 0.0021042),
                    ratio=c(1.211069/0.0021042, 0.0553729/0.0021042, 0.0021042/0.0021042))
kable(times, caption="Timings (seconds) of 5000 Calls to Distance Functions")
```


## Simple distance functions

Before jumping into [Network Adequacy] and [Nearest Neighbor] algorithms let's look at the core usage of the trilateration data structure and its use in simple distance functions.

What we mean by 'simple distance functions' is one of the following primitive functions common to SQL or map related software libraries:

* $D(p, q)$: returns the distance between points p and q
* $Within(d, q, P)$:  returns the set of all points in $P$ within distance $d$ of query point $q$
* $AnyWithin(d, q, P)$: returns a boolean result - True if $Within(d, q, P)$ is non-empty; False otherwise

### Distance Function

How can we use the Trilateration Index ($TI$) to improve the performance of a single distance function $D(p, q)$?  In the simplest case, we cannot... the construction of the $TI$ structures requires three distance functions to be calculated each for $p$ and $q$ (to the three fixed reference points).

However, for large datasets with fixed points where many distances need to be calculated between them, particularly if the distance function itself is computationally intensive (such as geospatial distances on an accurate ellipsoid model of earth) [@Lambert1942], we can use the $TI$ structure to create approximate distances, and provide upper and lower bounds on exact values.

For example, let's take our sample data:

```{r sampleDistance, fig.align="center", echo=FALSE}
data <- read.csv('../data/point_sample_10.csv')
ref <- read.csv('../data/sample_ref_points.csv')
ref <- ref[,2:4]
library(rdist)
data <- cbind(data, cdist(data, ref))[,2:6]
names(data) <- c('x', 'y', 'd1', 'd2', 'd3')
data <- data[order(data$d1),]
kable(data)
```

Here, X and Y are euclidean cartesian coordinates, and d1, d2, d3 are the distances from these points to our three reference points respectively.  See [2-D Bounded Example] for more details on the construction.  Note that in this case we have sorted the data by $d1$ -- this is essential, and incurs only $O(n*log(n))$ overhead.  This equates to how database indexes or arrays will hold the data in memory.


### Distance between two points
If we compare points 1 and 2 here (lines 4 and 2 in the $d1$-sorted table), what can we say about those two points' distances without invoking a distance function?  If we compare the distances, we can put lower bounds on their proximity using a direct, simple application of the triangle inequality.  For example $|d1(P_1) - d1(P_2)| = |54.19130 - 43.87912| = 10.33$ which means the points can be **_no closer than_** $10.33$ units to one another.   Similarly with d2 and d3, we get $|58.76869 - 40.89001| = 17.88$ and $56.13720 - 48.68666 = 7.45054$.  So now, the points can be no closer than $17.88$ units, although they are closer relative to the $d1$ and $d3$ points.

### Within/AnyWithin Distance

It's similarly easy to use this mechanism to approximate answers to "which points are within distance $d$ of query point $Q$?" and, relatedly, "is there at least one point in $P$ within distance $d$ to point $Q$?".

Looking back at our table, let's examine the question "which points are within distance 20 of point 5?".  Point 5 has coordinates $(86.12714, 52.201894)$, and is $63.60981$ units from $d1$.  Since we've stored the list sorted by $d1$, we can instantly limit our search to a sequential walk from points between $43.60981$ and $83.60981$ -- that is, points $(7, 1, 4, 9, 6)$ (excluding 5 itself).  This is, immediately, a 50% reduction in the dataset.

While performing the walk, we look for $d2$ between $24.08779 \pm 20$ and $d3$ between $82.65970 \pm 20$.  $d2$ rules out points $(7, 4, 6)$ and $d3$ rules out $(1)$, leaving only $(9)$ for consideration. To be completely certain, we can calculate $d = \sqrt{(86.12714 - 85.13531)^2 + (64.090063 - 78.065907)^2} =$ `r sqrt((86.12714 - 85.13531)^2 + (64.090063 - 78.065907)^2)` which is, indeed, within 20.

In pseudocode:
```
Within(d, P, TI):
   lowi = lowest i such that TI[i, d1] > P[d1] - d
   highi = highest i such that TI[i, d1] < P[d1] + d
   FOR i FROM lowi to highi:
       if TI[i, dx] between P[dx] - d and P[dx] + d for all x:
          ADD i to CANDIDATES
   FOR c in CANDIDATES:
       if D(c, P) < d:
          ADD c to RESULTS
   RETURN RESULTS
```

If we were answering the "is there at least one point..." version, it would be easy to shortcut the sequential walk when a match is reached.

### Alternate Order Indexes

For an additional possible performance improvement, we can create alternate indexes which store the data in sorted order along $d2$ and $d3$ (or any/all distances for arbitarty dimensions).  We search for the low and high indexes as before, but now we do so along each sorted index (for distances to each reference point).  Once we have the lists of individual candidates from each index, we need to find any point that is common to all candidate lists.  In practice we have not seen this behave as effectively as the single-index function, but this seems to come down to the cost of merging n-lists to find common elements.

In pseudocode:
```
WithinMulti(d, P, TI):
   FOR each ref point rx:
      lowi = lowest i such that TI[i, dx] > P[dx] - d
      highi = highest i such that TI[i, dx] < P[dx] + d
      FOR i FROM lowi to highi:
          if TI[i, dx] between P[dx] - d and P[dx] + d:
             ADD i to CANDIDATES[x]
    FOR c in CANDIDATES[1]:
      if c in CANDIDATES[x] for all x:
          ADD c to POSSIBLE
    FOR c in POSSIBLE:
       if D(c, P) < d:
          ADD c to RESULTS
    RETURN RESULTS
```