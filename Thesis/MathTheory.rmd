---
title: "Math Theory"
author: "Chip Lynch"
date: "1/31/2021"
output: pdf_document
---


## Underlying Theory Concepts and Problem Statement

The benefit of storing geographic points as a set of trilateration distances rather than latitude and longitude boils down to the simplification of comparing distances between points by shortcutting complex distance queries using simple subtractions.  We discuss the math behind the geospatial queries, to exhibit their complexity, and set some theoretical bounds on quick distance calculations using the trilateration index.


### Problem: High Cost of Geospatial Calculations

Calculating the distance between two points around the globe with precision is required for Satellite Communications and Geospatial Positioning Systems (GPS), as well as for ground based surveying and generally all applications requiring precise (sub-meter) measurements accounting for the curvature of the earth.[@ASPRS2015]  Recall from our research that modern methods of calculating accurate distances on a properly modeled (elliptical) earth requires calculating a converging series with an iterative algorithm.  This is significantly more computationally expensive than closed form mathematical distance functions like the Haversine spherical distance or more common Euclidean distances.

To get an idea of the relative complexity, we ran some basic timings using widely available python libraries that perform both calculations.  The Haversine is about 22 times faster than Karney's iterative approach.  For comparison, we include Euclidean functions, which are of course computationally simple, although their usefulness on curved surfaces are minimal, and subtraction, which our algorithms use as a bounding function in place of exact results, thus leveraging this timing delta:

```{r geodist_timings, echo=FALSE, results ='asis'}
library(knitr)
times <- data.frame(title=c("Geodesic", "Haversine", "Euclidean", "Subtraction"),
                    time=c(1.211069, 0.0553729, 0.0021042, 0.0012876),
                    ratio=c(1.211069/0.0021042, 0.0553729/0.0021042, 0.0021042/0.0021042, 0.0012876/0.0021042))
kable(times, caption="Timings (seconds) of 5000 Calls to Distance Functions")
```


## Simple Multilateration Index Operations

Before jumping into [Network Adequacy] and [Nearest Neighbor] algorithms let's look at the core usage of the trilateration data structure and its use in simple distance functions.

What we mean by 'simple distance functions' is one of the following primitive functions common to SQL or map related software libraries:

* $D(p, q)$: returns the distance between points p and q
* $Within(d, q, P)$:  returns the set of all points in $P$ within distance $d$ of query point $q$
** $CountWithin(d, q, P)$:  returns the count of the set of points from $Within(d, q, P)$
* $AnyWithin(d, q, P)$: returns a boolean result - True if $Within(d, q, P)$ is non-empty; False otherwise

### Distance Function

How can we use the Trilateration Index ($TI$) to improve the performance of a single distance function $D(p, q)$?  In the simplest case, we cannot... the construction of the $TI$ structures requires three distance functions to be calculated each for $p$ and $q$ (to the three fixed reference points).

However, for large datasets with fixed points where many distances need to be calculated between them, particularly if the distance function itself is computationally intensive (such as geospatial distances on an accurate ellipsoid model of earth) [@Lambert1942], we can use the $TI$ structure to create approximate distances, and provide upper and lower bounds on exact values.

For example, let's take our sample data:

```{r sampleDistance, fig.align="center", echo=FALSE}
data <- read.csv('../data/point_sample_10.csv')
ref <- read.csv('../data/sample_ref_points.csv')
ref <- ref[,2:4]
library(rdist)
data <- cbind(data, cdist(data, ref))[,c(2,3,6,5,4)]
names(data) <- c('x', 'y', 'd1', 'd2', 'd3')
data <- data[order(data$d1),]
kable(data)
```

Here, X and Y are euclidean cartesian coordinates, and d1, d2, d3 are the distances from these points to our three reference points respectively.  See [2-D Bounded Example] for more details on the construction.  Note that in this case we have sorted the data by $d1$ -- this is essential, and incurs only $O(n*log(n))$ overhead.  This equates to how database indexes or arrays will hold the data in memory.


### Distance between two points
If we compare points 1 and 2 here (lines 4 and 2 in the $d1$-sorted table), what can we say about those two points' distances without invoking a distance function?  If we compare the distances, we can put lower bounds on their proximity using a direct, simple application of the triangle inequality.  For example $|d1(P_1) - d1(P_2)| = |54.19130 - 43.87912| = 10.33$ which means the points can be **_no closer than_** $10.33$ units to one another.   Similarly with d2 and d3, we get $|58.76869 - 40.89001| = 17.88$ and $56.13720 - 48.68666 = 7.45054$.  So now, the points can be no closer than $17.88$ units, although they are closer relative to the $d1$ and $d3$ points.

### Within/AnyWithin Distance

It's similarly easy to use this mechanism to approximate answers to "which points are within distance $d$ of query point $Q$?" and, relatedly, "is there at least one point in $P$ within distance $d$ to point $Q$?".

Looking back at our table, let's examine the question "which points are within distance 20 of point 5?".  Point 5 has coordinates $(86.12714, 52.201894)$, and is $63.60981$ units from $d1$.  Since we've stored the list sorted by $d1$, we can instantly limit our search to a sequential walk from points between $43.60981$ and $83.60981$ -- that is, points $(7, 1, 4, 9, 6)$ (excluding 5 itself).  This is, immediately, a 50% reduction in the dataset.

While performing the walk, we look for $d2$ between $24.08779 \pm 20$ and $d3$ between $82.65970 \pm 20$.  $d2$ rules out points $(7, 4, 6)$ and $d3$ rules out $(1)$, leaving only $(9)$ for consideration. To be completely certain, we can calculate $d = \sqrt{(86.12714 - 85.13531)^2 + (64.090063 - 78.065907)^2} =$ `r sqrt((86.12714 - 85.13531)^2 + (64.090063 - 78.065907)^2)` which is, indeed, within 20.

In pseudocode:
```{r within_pseudocode, eval=FALSE, attr.source='.numberLines'}
Within(d, P, TI):
   lowi = lowest i such that TI[i, d1] > P[d1] - d
   highi = highest i such that TI[i, d1] < P[d1] + d
   FOR i FROM lowi to highi:
       if TI[i, dx] between P[dx] - d and P[dx] + d for all x:
          ADD i to CANDIDATES
   FOR c in CANDIDATES:
       if D(c, P) < d:
          ADD c to RESULTS
   RETURN RESULTS
```

$CountWithin$ is simply the same code but returning the count of $RESULTS$ not the points themselves.

If we were answering the "is there at least one point..." $AnyWithin$ version, it would be easy to shortcut the sequential walk when a match is reached.



### Alternate Order Indexes

For an additional possible performance improvement, we can create alternate indexes which store the data in sorted order along $d2$ and $d3$ (or any/all distances for arbitarty dimensions).  We search for the low and high indexes as before, but now we do so along each sorted index (for distances to each reference point).  Once we have the lists of individual candidates from each index, we need to find any point that is common to all candidate lists.  In practice we have not seen this behave as effectively as the single-index function, but this seems to come down to the cost of merging n-lists to find common elements.

In pseudocode:
```{r withinmulti_pseudocode, eval=FALSE, attr.source='.numberLines'}
WithinMulti(d, P, TI):
   FOR each ref point rx:
      lowi = lowest i such that TI[i, dx] > P[dx] - d
      highi = highest i such that TI[i, dx] < P[dx] + d
      FOR i FROM lowi to highi:
          if TI[i, dx] between P[dx] - d and P[dx] + d:
             ADD i to CANDIDATES[x]
    FOR c in CANDIDATES[1]:
      if c in CANDIDATES[x] for all x:
          ADD c to POSSIBLE
    FOR c in POSSIBLE:
       if D(c, P) < d:
          ADD c to RESULTS
    RETURN RESULTS
```

### Time Complexity
#### Estimation of Time Complexity of "Within" query

Note that lines 2 and 3 are $O(log_2(n))$ operations, since we can do a binary search on the sorted $TI$ structure to find points closest to a specific value.  The loop in line 4 is a sequential walk along the array; the time complexity being subject to the distance $d$ and the composition of the points $P$.

To estimate time complexity in random or average cases, let's take a closer look at what's happening with some visual elements.  Looking at the "Within Complexity" Figure, we see the following:

* Three reference points $r_1..r_3$ as triangles
* 10 data points $p_1..p_{10}$ as numbered small circles
* The query point $q$, a small square
* A dashed circle with radius $d$.  Note that, in this example, point 2 lies precisely $d$ units from $q$, with $d \approx 22.47$
* Dashed lines from $r_1$ to each point indicating their distance $p_{d1}$ (and sort order in $TI$) from $r_1$
* A large ring representing all of the space where a point within $d$ of $q$ must lie, with respect to $r_1$

```{r within_complexity, echo=FALSE, fig.cap="Within time complexity illustration", fig.height=4, fig.width=4}
library(ggplot2)
library(ggforce)

myDist <- function(x, y) {
  # x and y must be 2D points -- (x,y) tuples i.e.: x <- c(1,2) y <- c(3,4)
  d <- sqrt((x[[1]]-y[[1]])^2+(x[[2]]-y[[2]])^2)
  return(d)
}


data <- read.csv('../data/point_sample_10.csv')
data <- data[,-1]

refpoints <- read.csv('../data/sample_ref_points.csv')
refpoints <- refpoints[c(3,2,1),-1]
row.names(refpoints) <- c(1,2,3)
names(refpoints) <- c('A', 'x', 'y')
refpoints$A <- c(1,2,3)

querypoint <- data.frame(array(c(50, 65), dim=c(1,2)))
names(querypoint) <- c('x', 'y')

ref_lines = data.frame(x=refpoints[1, 'x'], y=refpoints[1,'y'], xend=data[,'x'], yend=data[,'y'])

r3dist = apply(ref_lines, 1, function(x) sqrt((x[3]-x[1])^2 + (x[4]-x[2])^2))
data$r3dist <- r3dist
data <- data[order(data$r3dist),]

r1dist = apply(data, 1, function(x) sqrt((x[1] - refpoints[1,'x'])^2 + (x[2] - refpoints[1, 'y'])^2))
data$r1dist <- r1dist

r2dist = apply(data, 1, function(x) sqrt((x[1] - refpoints[2,'x'])^2 + (x[2] - refpoints[2, 'y'])^2))
data$r2dist <- r2dist

qdists = apply(refpoints, 1, function(x) sqrt((querypoint$x - x[2])^2 + (querypoint$y - x[3])^2))
qdists <- data.frame(array(qdists, dim=c(1,3)))
names(qdists) <- c('r1dist', 'r2dist', 'r3dist')
querypoint <- cbind(querypoint, qdists)

qp2dist <- myDist(c(querypoint$x, querypoint$y), c(data[rownames(data)==2, 'x'], data[rownames(data)==2, 'y']))
ggplot() +
  geom_arc_bar(data=refpoints[1,],
               aes(x0=x,y0=y,
                   r0=querypoint$r1dist - qp2dist,
                   r=querypoint$r1dist + qp2dist,
                   start=-pi/2, end=1.5*pi, fill=TRUE)) +
  geom_point(data=data[rownames(data) %in% c(1,7,4,6),], mapping=aes(x=x, y=y), color="blue", size=2) +
  geom_point(data=data[!rownames(data) %in% c(1,7,4,6),], mapping=aes(x=x, y=y), color="black", size=2) +
  geom_text(data=data, aes(x=x, y=y, label=rownames(data)), nudge_y = -5, color="blue") +
  geom_point(data=refpoints[0:3,], mapping=aes(x=x, y=y), color="red", shape="triangle", size=3) +
  geom_text(data=refpoints[0:3,], aes(x=x, y=y, label=A), nudge_y = -5) +
  geom_point(data=querypoint, mapping=aes(x=x,y=y), color="green4", shape="square", size=3) +
  geom_point(data=data[rownames(data)==2,], aes(x=x, y=y), color="orange", shape="diamond", size=4) +
  #  geom_point(data=data[rownames(data) %in% c(4, 7),], aes(x=x, y=y), color="pink", size=4) +
  geom_circle(aes(x0=x, y0=y, r=qp2dist), data=querypoint, linetype="dashed") +
  geom_segment(data=ref_lines, mapping=aes(x=x, y=y, xend=xend, yend=yend), linetype="dotted") +
  coord_fixed(ratio = 1, xlim=c(0,120), ylim=c(-5, 100)) +
  theme(legend.position = "none")
```


Comparing this to our sample data, the length of the dashed lines correspond to the sort order in the $TI$ index.  That is, $8, 4, 6, 2, 7, 1, 9, 5, 3, 10$ are increasingly distant from $r_1$.  This is how we quickly prune the list -- points within $d$ distance of $q$ are, by definition, between $q_{d1}-d$ and $q_{d1}+d$ distance from $r_1$ - namely, points $4, 6, 2, 7, 1$.  The rest can be no closer than this.  In this case, all the points in the ring happen to also be within $d$ of $q$, but of course that will not always be the case.

Clearly there are antagonistic datasets that would thwart any benefit.  If the data were arranged in a circle all near distance $d$ from $r_1$, we would have culled no points from this approach.  In that particular case, the alternate $WithinMulti$ construction would be beneficial.

If the points were randomly distributed, however, then the expected culling ratio of the index becomes the area of the ring divided by the area of the total point space.  On an infinite plane, of course, this approaches zero, but in finite spaces, such as this example (and geospatial coordinates), we can calculate the area.  Here, we use a common Monte Carlo method to estimate that, for this distance, on the $100x100$ grid, the ring covers about 48.7% of the area.[@Wasserstein1989]  Given that we had to check five of our ten points, this seems reasonable.

\newpage

#### Time complexity of WithinMulti

If, rather than cull by one ref point then search the remainder, we cull by all three (the $WithinMulti$ approach), we see something different:

```{r withinmulti_complexity, echo=FALSE, fig.cap="WithinMulti time complexity illustration", fig.height=4, fig.width=4}
qp6dist <- myDist(c(querypoint$x, querypoint$y), c(data[rownames(data)==6, 'x'], data[rownames(data)==6, 'y']))

ggplot() +
  geom_arc_bar(data=refpoints[3,],
               aes(x0=x,y0=y,
                   r0=querypoint$r3dist - qp6dist,
                   r=querypoint$r3dist + qp6dist,
                   start=-pi/2, end=1.5*pi, fill=TRUE)) +
  geom_arc_bar(data=refpoints[2,],
               aes(x0=x,y0=y,
                   r0=querypoint$r2dist - qp6dist,
                   r=querypoint$r2dist + qp6dist,
                   start=pi/2, end=-1.5*pi, fill=TRUE, alpha=0.5)) +
  geom_arc_bar(data=refpoints[1,],
               aes(x0=x,y0=y,
                   r0=querypoint$r1dist - qp6dist,
                   r=querypoint$r1dist + qp6dist,
                   start=-pi/2, end=1.5*pi, fill=TRUE, alpha=0.5)) +
  geom_point(data=data[rownames(data) %in% c(1,4),], mapping=aes(x=x, y=y), color="blue", size=2) +
  geom_point(data=data[!rownames(data) %in% c(1,4),], mapping=aes(x=x, y=y), color="black", size=2) +
  geom_point(data=refpoints[0:3,], mapping=aes(x=x, y=y), color="red", shape="triangle", size=3) +
  geom_text(data=refpoints[0:3,], aes(x=x, y=y, label=A), nudge_y = -2) +
  geom_point(data=querypoint, mapping=aes(x=x,y=y), color="green4", shape="square", size=3) +
  geom_point(data=data[rownames(data)==6,], aes(x=x, y=y), color="orange", shape="diamond", size=4) +
  geom_text(data=data, aes(x=x, y=y, label=rownames(data)), nudge_y = -2, color="blue") +
  #  geom_point(data=data[rownames(data) %in% c(4, 7),], aes(x=x, y=y), color="pink", size=4) +
  geom_circle(aes(x0=x, y0=y, r=qp6dist), data=querypoint, linetype="dashed") +
  # geom_segment(data=ref_lines, mapping=aes(x=x, y=y, xend=xend, yend=yend), linetype="dotted") +
  coord_fixed(ratio = 1, xlim=c(0,120), ylim=c(-5, 100)) +
  theme(legend.position = "none")
```

This time we've used a slightly smaller $d$ - the distance from $q$ to $p_6$ or $d \approx 16.127$, which helps illustrate excluded points $2, 7$.

To perform this efficiently, we would need to store additional sorted lists in memory - namely the points $P$ sorted along distances from $r_2$ and $r_3$ respectively... for example the index with respect to $r_2$ could look like this (x and y need not even be stored again to save more memory, as long as the point index is available):

```{r r2_index, echo=FALSE}
data <- read.csv('../data/point_sample_10.csv')
ref <- read.csv('../data/sample_ref_points.csv')
ref <- ref[,2:4]
library(rdist)
data <- cbind(data, cdist(data, ref))[,c(2,3,5)]
names(data) <- c('x', 'y', 'd2')
data <- data[order(data$d2),]
kable(data, caption="Indexed sample data with respect to r2")
```


Here, culling points along the three distances respectively we get:

* $r_1$: $4, 6, 2, 7, 1$
* $r_2$: $1, 7, 6, 2, 4$
* $r_3$: $1, 4, 6$

It's clear that $r_3$ is the most restrictive; examining the images, it's easy to see why... points 2 and 7 reside within the rings aroudn $r_1$ and $r_2$, but not that of $r_3$.   The final step, of finding which elements are common to these three lists is, unfortunately, not efficient.  We can start with the smallest list, which is some help, but as these are sorted in different orders, finding the intersection of all of the lists is, algorithimcally, non-trivial.  Given $m$ sets containing $N$ elements total which result in $o$ points in the intersection, modern research indicates space complexity of at least $O(N \log(N))$ and time complexity $O(\sqrt{N*o} + o))$.[@Cohen2010]


Still, the improvement in the initial cull may be worth it.  Recall that, using a single reference point, the time complexity was a result of the area of the ring divided by the area of the space.  In the multi-reference-point version, with $m$ reference points, the result is $m$ times the area of the intersection of the $m$ rings, divided by the area of the space.

\newpage

#### Numerical Approximation:
In this current example, with $d \approx 16.127$, we can numerically approximate this value:

* total area: $100x100 = 10000$
* $r_1$ ring area is $\approx 33.1\%$
* $r_2$ ring area is $\approx 32.7\%$
* $r_3$ ring area is $\approx 37.9\%$
* Intersection of all three rings has area $\approx 9.4\%$

A Monte Carlo simulation of this is illustrated in Figure: "Monte Carlo Estimating Ring Overlap Area".

```{r space_estimation, echo=FALSE, fig.cap="Monte Carlo Estimating Ring Overlap Area", fig.height=4, fig.width=4}

library(ggplot2)
library(ggforce)

myDist <- function(x, y) {
  # x and y must be 2D points -- (x,y) tuples i.e.: x <- c(1,2) y <- c(3,4)
  d <- sqrt((x[[1]]-y[[1]])^2+(x[[2]]-y[[2]])^2)
  return(d)
}


data <- read.csv('../data/point_sample_10.csv')
data <- data[,-1]

refpoints <- read.csv('../data/sample_ref_points.csv')
refpoints <- refpoints[c(3,2,1),-1]
row.names(refpoints) <- c(1,2,3)
names(refpoints) <- c('A', 'x', 'y')
refpoints$A <- c(1,2,3)

querypoint <- data.frame(array(c(50, 65), dim=c(1,2)))
names(querypoint) <- c('x', 'y')

qdists = apply(refpoints, 1, function(x) sqrt((querypoint$x - x[2])^2 + (querypoint$y - x[3])^2))
qdists <- data.frame(array(qdists, dim=c(1,3)))
names(qdists) <- c('r1dist', 'r2dist', 'r3dist')
querypoint <- cbind(querypoint, qdists)

test <- data.frame(x = runif(5000, min=0, max=100), y=runif(1000, min=0, max=100))


qp6dist <- myDist(c(querypoint$x, querypoint$y), c(data[rownames(data)==6, 'x'], data[rownames(data)==6, 'y']))

t1d <- apply(test, 1, function(x) sqrt((refpoints[1,'x'] - x[1])^2 + (refpoints[1, 'y'] - x[2])^2))
t2d <- apply(test, 1, function(x) sqrt((refpoints[2,'x'] - x[1])^2 + (refpoints[2, 'y'] - x[2])^2))
t3d <- apply(test, 1, function(x) sqrt((refpoints[3,'x'] - x[1])^2 + (refpoints[3, 'y'] - x[2])^2))

low1 = qdists$r1dist - qp6dist
high1 = qdists$r1dist + qp6dist
low2 = qdists$r2dist - qp6dist
high2 = qdists$r2dist + qp6dist
low3 = qdists$r3dist - qp6dist
high3 = qdists$r3dist + qp6dist

r1in <- test[which(t1d >= low1 & t1d <= high1),]
r2in <- test[which(t2d >= low2 & t2d <= high2),]
r3in <- test[which(t3d >= low3 & t3d <= high3),]
tin <- test[which(t1d >= low1 & t1d <= high1 & t2d >= low2 & t2d <= high2 & t3d >= low3 & t3d <= high3),]
tout <- test[which(t1d < low1 | t1d > high1 | t2d < low2 | t2d > high2 | t3d < low3 | t3d > high3),]

ggplot() +
  geom_point(data=tout, mapping=aes(x=x, y=y), color="black", size=1) +
  geom_arc_bar(data=refpoints[3,],
               aes(x0=x,y0=y,
                   r0=querypoint$r3dist - qp6dist,
                   r=querypoint$r3dist + qp6dist,
                   start=-pi/2, end=1.5*pi, fill=TRUE, alpha=0.5)) +
  geom_arc_bar(data=refpoints[2,],
               aes(x0=x,y0=y,
                   r0=querypoint$r2dist - qp6dist,
                   r=querypoint$r2dist + qp6dist,
                   start=pi/2, end=-1.5*pi, fill=TRUE, alpha=0.5)) +
  geom_arc_bar(data=refpoints[1,],
               aes(x0=x,y0=y,
                   r0=querypoint$r1dist - qp6dist,
                   r=querypoint$r1dist + qp6dist,
                   start=-pi/2, end=1.5*pi, fill=TRUE, alpha=0.5)) +
  geom_point(data=refpoints[1:3,], mapping=aes(x=x, y=y), color="red", shape="triangle", size=3) +
  geom_circle(aes(x0=x, y0=y, r=qp6dist), data=querypoint, linetype="dashed") +
  geom_point(data=querypoint, mapping=aes(x=x,y=y), color="green4", shape="square", size=3) +
  geom_point(data=tin, mapping=aes(x=x, y=y), color="green", size=1) +
  coord_fixed(ratio = 1, xlim=c(0,120), ylim=c(-5, 100)) +
  theme(legend.position = "none")
```

Again, these numbers will vary with $d$ more than anything else, but the concept here is that for low $d$, the area of the ring can be significantly less than that of the entire search area, even moreso the intersection of the area of three such rings, benefitting the actual time complexity of the query.
