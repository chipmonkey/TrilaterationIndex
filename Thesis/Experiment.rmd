---
title: "Experiment"
author: "Chip Lynch"
date: "3/13/2021"
output:
  pdf_document: default
  html_document: default
---

# Experimentation

## Experimental Setup

We plan experiments for Network Adequacy in SQL databases, and Nearest Neighbor applications using Python.

We describe the experiment, setup, process, and results here in detail, but note that our [Appendix] includes links to the specific code and sample data used, which should answer any particular implementation detail.

### Experiment 1: Python Nearest Neighbors with ANN

1. We adapt the popular Python package scikit-learn to execute our Trilateration Index and perform nearest-neighbor search.
2. We adapt the ann-benchmarks software, which is designed explicitly for comparing performance of nearest neighbor algorithms[@Amueller2020], to record results.  This includes the addition of a geodesic dataset and distance function (ann-benchmarks only supported jaccard, hamming, euclidean, and angular distance functions) for consistency (see [Experimental data - Geodesic query points] for details).
3. We implement the four algorithms described in [Multilateration NN Algorithms], namely "Trilateration" (TRI), "TrilaterationApprox" (TIA), "TrilaterationExpand" (TIE), and "TrilaterationExpand2" (TIE2)
4. We execute the ann-benchmarks software against our and other modern $NN$ algorithms, and record the results in [ANN-Benchmarks]

Note that we use a widely available geodesic distance implementation from Geopy to provide a consistent distance function implementation, thus avoiding any bias in results due to differences between implementations.[@geopy]  Since the geodesic distance function is the result of a convergent series, it is possible to vary the precision of the calculation, trading performance for accuracy.  We use the default settings in the Geopy implementation, again for a consistent comparison across $NN$ techniques.  

### Experiment 2: SQL Network Adequacy

For Network Adequacy, no standard benchmark exists (such as ann-benchmarks for Nearest Neighbor), so our experimental setup requires a bit more setup.  We use the following steps:

1. We take the same 150,000 point geospatial data set used in the $NN$ experiment (see [Experimental data - Geodesic query points] for details) and assign the points randomly to 15 categories of varying sizes (see table [Record Counts by Category ID]).

```{r categories, echo=FALSE}
library(knitr)
cats <- read.csv('../data/lat_long_categorized.csv')
cats[cats$Category < 10,'Category'] <- '0-9'
cats$Category <- as.factor(cats$Category)
results <- t(table(cats$Category))
results[1] <- results[1] / 10
row.names(results) <- c('Record Count')
kable(results, caption="Record Counts by Category ID")
```

2. We implement our two Network Algorithms (NAIVE-NA and TRILAT-NA) described in [Multilateration NA Algorithms] in the SQL database.
3. We execute these over various combinations of categories in our dataset, logging the duration of each combination for comparison.

Note that the above categories are used to simulate the answers to the questions we had from our healthcare examples... rather than "90% of the members must live within 50 miles of a covered emergency room", we ask if "90% of the points in category 10 are within 50 miles of a point in category 13".  The charts in our [Result Charts](#Result-Charts) section effectively describe the performance of calculating the percent for various distances for these sorts of questions.


### Experimental data - Geodesic query points:

We have created a dataset specifically to test Geodesic queries.  The dataset is a synthetic set of 150,000 geospatial points spread across and near Kentucky with roughly the distribution of the population.

```{r kentucky_map, echo=FALSE, fig.cap="A set of 150,000 Geodesic sample data points based on population density in the US state of Kentucky.  Points are semi-transparent so darker areas display higher density of point data.", fig.width=6, fig.height=4, dev='png'}
library('maps')
library('mapproj')
library(ggplot2)

m = map("state", fill = TRUE, plot = FALSE)
usa <- map_data("usa")
states <- map_data("state")
kentucky <- subset(states, region %in% "kentucky")
sampleLL <- read.csv('../data/lat_long_synthetic.csv')
sampleLL$Color = 'black'
minlong <- min(kentucky$long)
minlat <- min(kentucky$lat)
maxlong <- max(kentucky$long)
maxlat <- max(kentucky$lat)
x <- which (sampleLL$Longitude > minlong &
                   sampleLL$Longitude < maxlong &
                   sampleLL$Latitude > minlat &
                   sampleLL$Latitude < maxlat)
sampleLL <- sampleLL[x,]

counties <- map_data('county')
ky_counties <- subset(counties, region == "kentucky")
ggplot() + 
  geom_polygon(data=kentucky, aes(x = long, y = lat, group = group), fill = "white", color = "black") + 
  coord_fixed(1.3) +
  #  Not sure why I can't get these next two lines to work in one:
  geom_point(data=sampleLL[sampleLL$Color=='black',], color='black', shape=16,
             aes(x=Longitude, y=Latitude), alpha = 0.1, size=1) +
  geom_polygon(data=ky_counties, aes(x=long, y=lat, group=group), fill = NA, color = "white")
```



\newpage

```{r child = 'NearestNeighbor.rmd'}
```


\newpage

```{r child = 'NetworkAdequacyExperiment.rmd'}
```


\newpage

## Experimental Results

We implemented our new algorithms in Cython/Python and SQL as described in previous sections.  The source code used for this analysis is linked in the [Appendix].

```{r child = 'ANN_Benchmarks.rmd'}
```

\newpage

```{r child = 'SQL_Results.rmd'}
```

