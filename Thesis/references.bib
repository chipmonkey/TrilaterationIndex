@article{Wishner2017,
   abstract = {Since the launch of the health insurance marketplaces under the Affordable Care Act (ACA), many health insurers have competed for customers by offering plans with narrow provider networks at lower costs than plans with broader networks. All marketplace health plans are required to provide their enrollees with access to covered services " without unreasonable delay, " but the increased reliance on narrow provider networks has raised concerns over whether consumers have timely access to needed health care. Previous studies have analyzed the different network adequacy standards states use to regulate provider networks; this study addresses the systems state regulators use to analyze, evaluate, and monitor compliance with network adequacy standards. These systems likely will become increasingly important. In February 2017, the Trump administration proposed to loosen federal network adequacy standards in the federal marketplace and delegate more responsibility to state regulators for network adequacy review. Some states may respond by adapting their network adequacy standards and systems to address the evolving nongroup market. Regardless of what system emerges from the initiative to repeal and replace the Affordable Care Act, insurers are expected to continue to rely on narrow provider networks to compete for customers in the nongroup market. 1 With less federal regulation of the nongroup market, state regulation of network adequacy will become increasingly significant and more states may consider whether to increase their regulation and oversight of provider networks. As states consider how to ensure that consumers get timely access to necessary care through increasingly narrow provider networks, this study offers some cross-cutting observations from four states with experience overseeing health plan compliance with network adequacy standards: California, Colorado, Illinois, and Nevada. Collectively the four study states have many years of experience analyzing, monitoring, and enforcing network adequacy standards. Although their standards and systems for reviewing compliance with those standards differ, some common themes may provide lessons for policymakers and regulators nationwide: • Quantitative standards and related metrics help regulators evaluate network adequacy, but regulators need flexibility in applying those standards. • A combination of standardized forms and narrative submissions help regulators analyze network adequacy. • States vary in the extent to which regulators require insurers to change or supplement proposed networks. • States vary in their transparency about insurer network submissions and regulators' review of those submissions. • Challenges remain in assessing and monitoring network adequacy. These include » ensuring the accuracy of provider directories, With support from the Robert Wood Johnson Foundation (RWJF), the Urban Institute is undertaking a comprehensive monitoring and tracking project to examine the implementation and effects of the Patient Protection and Affordable Care Act of 2010 (ACA). The project began in May 2011 and will take place over several years. The Urban Institute will document changes to the implementation of national health reform to help states, researchers and policymakers learn from the process as it unfolds. Reports that have been prepared as part of this ongoing project can be found at www.rwjf.org and www.healthpolicycenter.org. ACA Implementation—Monitoring and Tracking 3 » addressing the needs of rural communities, » evaluating network adequacy when multiple plans count the same providers to meet state quantitative standards, and » strengthening the systems for gathering and using consumer complaints and grievances to monitor and identify current network adequacy problems. As the nongroup market evolves, state standards and state regulators will become increasingly important in ensuring that health plans' provider networks meet the health care needs of their enrollees.},
   author = {Jane B Wishner and Jeremy Marks},
   issue = {March},
   journal = {Robert Wood Johnson Foundation},
   title = {Ensuring Compliance with Network Adequacy Standards: Lessons from Four States},
   year = {2017},
}

@article{Mahdavi2011,
   abstract = {In this research, STNEP is being studied considering network adequacy and limitation of investment cost by decimal codification genetic algorithm (DCGA). The goal is obtaining the maximum of network adequacy with lowest expansion cost for a specific investment. Finally, the proposed idea is applied to the Garvers 6-bus network. The results show that considering the network adequacy for solution of STNEP problem is caused that among of expansion plans for a determined investment, configuration which has relatively lower expansion cost and higher adequacy is proposed by GA based method. Finally, with respect to the curve of adequacy versus expansion cost it can be said that more optimal configurations for expansion of network are obtained with lower investment costs.},
   author = {Meisam Mahdavi and Elham Mahdavi},
   doi = {10.5281/zenodo.1086125},
   issn = {2010376X},
   journal = {World Academy of Science, Engineering and Technology},
   title = {Transmission expansion planning considering network adequacy and investment cost limitation using genetic algorithm},
   volume = {80},
   year = {2011},
}

@article{Ahmadi2019,
   abstract = {With the evolution of smart grids, penetration of distributed energy resources (DERs) in the distribution networks has become ever-increasing problem. To improve network reliability, the complexity of the two important aspects of adequacy and security must be well assessed. There is a trade-off between adequacy of DERs, and the distribution network security, i.e. improving the adequacy can reduce the security. In this study, enhancement of the distribution network adequacy and security is proposed. In this regard, capacity of simultaneous reconfiguration and DERs sizing are utilised to improve the adequacy and security of an active distribution network. In the reconfiguration process, graph theory concept is adopted to implement a fast reconfiguration method. Since DERs are active, a combined bus and line security index is used to overcome security concerns of their existence. The IEEE 33-bus distribution network as a widely used standard test system in reconfiguration studies, and a practical 83-bus distribution network of Taiwan Power Company (TPC) which is a part of a real distribution network, are used to test the performance of the proposed method. The simulation results demonstrate the performance of the proposed framework.},
   author = {Seyed Alireza Ahmadi and Vahid Vahidinasab and Mohammad Sadegh Ghazizadeh and Kamyar Mehran and Damian Giaouris and Phil Taylor},
   doi = {10.1049/iet-gtd.2019.0824},
   issn = {17518687},
   issue = {20},
   journal = {IET Generation, Transmission and Distribution},
   title = {Co-optimising distribution network adequacy and security by simultaneous utilisation of network reconfiguration and distributed energy resources},
   volume = {13},
   year = {2019},
}

@article{Lambert1942,
   author = {Lambert, W. D},
   issue = {32 (5)},
   title = {The distance between two widely separated points on the surface of the earth},
   journal = {J. Washington Academy of Sciences},
   year = {1942}
}

@article{Johnson2017,
   abstract = {Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a novel design for k-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. It enables the construction of a high accuracy k-NN graph on 95 million images from the YFCC100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.},
   author={J. {Johnson} and M. {Douze} and H. {Jégou}},
   journal={IEEE Transactions on Big Data},
   title={Billion-scale similarity search with GPUs},
   year={2019},
   volume={},
   number={},
   pages={1-1},
   doi={10.1109/TBDATA.2019.2921572}}

@article{ASPRS2015,
   author = {ASPRS},
   abstract = {In the December 2013 PE&RS, ASPRS published a draft version of a new accuracy standard which was intended to address new technologies and to expand on and replace the 1990 Accuracy Standards for Large-Scale Maps. After several review periods and significant revisions, a final version of the new standard was adopted by the Board of Directors in November 2014. The new standard and development background can be found here: "ASPRS Positional Accuracy Standards for Digital Geospatial Data".},
   doi = {10.14358/pers.81.3.a1-a26},
   issn = {00991112},
   issue = {3},
   journal = {Photogrammetric Engineering & Remote Sensing},
   month = {2},
   pages = {1-26},
   publisher = {American Society for Photogrammetry and Remote Sensing},
   title = {ASPRS Positional Accuracy Standards for Digital Geospatial Data},
   volume = {81},
   year = {2015},
}

@article{gade2010, title={A Non-singular Horizontal Position Representation}, volume={63}, DOI={10.1017/S0373463309990415}, number={3}, journal={Journal of Navigation}, publisher={Cambridge University Press}, author={Gade, Kenneth}, year={2010}, pages={395–417}}

@article{Vincenty1975,
   author = {T. Vincenty},
   title = {DIRECT AND INVERSE SOLUTIONS OF GEODESICS ON THE ELLIPSOID WITH APPLICATION OF NESTED EQUATIONS},
   journal = {Survey Review},
   volume = {23},
   number = {176},
   pages = {88-93},
   year  = {1975},
   publisher = {Taylor & Francis},
   doi = {10.1179/sre.1975.23.176.88},
   URL = { 
           https://doi.org/10.1179/sre.1975.23.176.88
       
   },
   eprint = { 
           https://doi.org/10.1179/sre.1975.23.176.88
       
   },
    abstract = { AbstractThis paper gives compact formulae for the direct and inverse solutions of geodesics of any length. Existing formulae have been recast for efficient programming to conserve space and reduce execution time. The main feature of the new formulae is the use of nested equations for elliptic terms. Both solutions are iterative. }
}

@article{Karney2013,
   abstract = {Algorithms for the computation of geodesics on an ellipsoid of revolution are given. These provide accurate, robust, and fast solutions to the direct and inverse geodesic problems and they allow differential and integral properties of geodesics to be computed. © 2012 The Author(s).},
   author = {Charles F.F. Karney},
   doi = {10.1007/s00190-012-0578-z},
   issn = {14321394},
   issue = {1},
   journal = {Journal of Geodesy},
   title = {Algorithms for geodesics},
   volume = {87},
   year = {2013},
}

@article{Arriaga2019,
   abstract = {<p><strong>Abstract.</strong> The process of manually georeferencing or aligning historic or illustrated maps with contemporary maps can be a difficult and time consuming task (Fleet et al., 2012). It is generally accepted that the level of understanding necessary to correctly georeference a single image can be rather daunting (Bajcsy and Alumbaugh, 2003). This is especially challenging in an open environment where there is no previous information to help approximating the real coordinates.</p><p>Over the last couple of decades there have been advances in the automatic georeferencing of map images, aerial photographs or raster maps (Chen et al., 2004), (Desai et al., 2005), (Kim et al., 2010), (Cléry et al., 2014). However, there has been little discussion dealing with heterogeneous maps. For instance, some algorithms apply fixed image processing techniques to find features within the map images, and then try to match these patterns of features to a database of geographical information (Chen et al., 2004). The drawback with this approach is that the image processing operations used in a particular style may not work for a map created using a different style. Other techniques only work for a specific kind of map, like street maps (Desai et al., 2005) or aerial photographs (Kim et al., 2010). Furthermore, the artistic vision of the creator or the theme of the map can also result in these features being represented in different ways (Fiori, 2005). For instance, some styles or themes may highlight some roads or completely ignore others. Finally, historic but inaccurate cartography or contemporary illustrated maps can suffer from distortion or unusual perspective (Cajthaml, 2011).</p><p>In this paper, we present a novel algorithm to automatically help start the georeferencing of historic and illustrated maps based on the text found in the map image. To accomplish this, we leverage the power of modern OCR (Optical Character Recognition) and geocoding services on the cloud. The proposed algorithm is able to calculate the area covered by the map, and where north is located in the image, with a precision greater than 80%. This information obtained represents a great help to inexpert users performing the alignment and georeference of maps for the first time. We also propose an optional machine learning module to speed up the process in dynamic environments in which the time required to obtain a result is an important factor. Figure 1 shows some examples of heterogeneous maps processed with the proposed algorithm.</p><p>The proposed algorithm contains five modules as shown in Figure 2. The first module applies an OCR process to extract the text contained within the input image. The results pass through a processing step to filter the text using heuristics to remove incorrect and ambiguous entries. The next module (optional) is a bidirectional LSTM (Long shortterm memory) recurrent neural network (Graves and Schmidhuber, 2005) that takes text and orders it according to likelihood of useful geocoding result return. The third module takes the text (ordered or not) and searches for each line in a geocoding service. The output is a list of locations, each one with its real world latitude and longitude and its coordinates within the image. The fourth module calculates a matrix of distances between locations. Each distance contains the real life geodesic distance (Karney, 2013) in meters, the Euclidean distance between each piece of text in pixels, the calculated meters per pixel (MPP), and the rotation. We define rotation as the difference in angle between real life location and the text in the image. Using the MPP and rotation as dimensions, the module finds clusters of corresponding locations. Lastly, the largest cluster is selected as the best. The fifth and final module uses the best cluster of locations and calculates the georeference information. This output information contains the northeast and southwest corners of the map, a list of mapping points, as well as the angle of north in the image (counter-clockwise, where 0 degrees is pointing up).</p><p>The proposed algorithm has approximately twelve hyper-parameters that can be tuned. We found that one of the most important is the minimum size of the cluster used to calculate the georeference information. In other words, the minimum number of corresponding locations the algorithm needs to converge.</p><p>In Table 1 we show the results of executing the algorithm against a set of 359 illustrated maps obtained from Stroly’s database (Vermeulen et al., 2011). The maps were manually georeferenced, and this information is used as ground truth. The georeference information returned by the algorithm is considered correct when two conditions are met. First, the width of the calculated area is between 50% and 200% of the width of the real area. Second, there is an intersection between both areas. Figure 3 shows the visualization of some results, executing the algorithm against several kinds of maps. The map area delimited in blue is the ground truth, while the one in orange is the one calculated by the presented algorithm. The markers in blue are the locations that are part of the cluster used to calculate the information.</p><p>In conclusion, we offer a novel solution to start and in some cases to complete the georeferencing process for heterogeneous historic and illustrated maps based on the text contained within them. The algorithm does not need vector information or geographical databases, nor image preprocessing. We have proven that even with a small cluster of locations the precision of this method is greater than 80%. The precision increases when the hyper-parameter is set to need larger clusters to converge (98.86% for a minimum of six locations). In future iterations we aim to improve the algorithm to increase the precision for smaller clusters and to improve the recall in general.</p>},
   author = {Enrique J. Arriaga-Varela and Toru Takahashi},
   doi = {10.5194/ica-abs-1-15-2019},
   journal = {Abstracts of the ICA},
   title = {Automatic Georeferencing of Heterogeneous Historic and Illustrated Maps},
   volume = {1},
   year = {2019},
}

@article{Lambert1942,
    author={Lambert, W. D.},
    title = {The distance between two widely separated points on the surface of the earth},
    journal = {Journal of the Washington Academy of Sciences},
    publisher = {Washington Academy of Sciences},
    year = {1942},
    volume = {32},
    number = {5},
    pages = {125–130},
    ISSN = {00430439},
    URL = {http://www.jstor.org/stable/24531873},
}

@article{Amueller2020,
   title = {ANN-Benchmarks: A benchmarking tool for approximate nearest neighbor algorithms},
   journal = {Information Systems},
   volume = {87},
   pages = {101374},
   year = {2020},
   issn = {0306-4379},
   doi = {https://doi.org/10.1016/j.is.2019.02.006},
   url = {https://www.sciencedirect.com/science/article/pii/S0306437918303685},
   author = {Martin Aumüller and Erik Bernhardsson and Alexander Faithfull},
   keywords = {Benchmarking, Nearest neighbor search, Evaluation},
   abstract = {This paper describes ANN-Benchmarks, a tool for evaluating the performance of in-memory approximate nearest neighbor algorithms. It provides a standard interface for measuring the performance and quality achieved by nearest neighbor algorithms on different standard data sets. It supports several different ways of integrating k-NN algorithms, and its configuration system automatically tests a range of parameter settings for each algorithm. Algorithms are compared with respect to many different (approximate) quality measures, and adding more is easy and fast; the included plotting front-ends can visualize these as images, LaTeX plots, and websites with interactive plots. ANN-Benchmarks aims to provide a constantly updated overview of the current state of the art of k-NN algorithms. In the short term, this overview allows users to choose the correct k-NN algorithm and parameters for their similarity search task; in the longer term, algorithm designers will be able to use this overview to test and refine automatic parameter tuning. The paper gives an overview of the system, evaluates the results of the benchmark, and points out directions for future work. Interestingly, very different approaches to k-NN search yield comparable quality-performance trade-offs. The system is available at http://ann-benchmarks.com.}
}

@unknown{unknown,
author = {Wang, Hongya and Wang, Zhizheng and Wang, Wei and Xiao, Yingyuan and Zhao, Zeng and Yang, Kaixiang},
year = {2020},
month = {12},
pages = {},
title = {A Note on Graph-Based Nearest Neighbor Search}
}

@article{Tainiter1963,
   author = {M. Tainiter},
   doi = {10.1145/321172.321178},
   issn = {1557735X},
   issue = {3},
   journal = {Journal of the ACM (JACM)},
   title = {Addressing for Random-Access Storage with Multiple Bucket Capacities},
   volume = {10},
   year = {1963},
}

@article{Bentley1975,
   abstract = {This paper develops the multidimensional binary search tree (or k-d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O(log n); deletion of the root, O(n(k-1)/k); deletion of a random node, O(log n); and optimization (guarantees logarithmic performance of searches), O(n log n). Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O(n(k-t)/k)] and for nearest neighbor queries [empirically observed average running time of O(log n).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that k-d trees could be quite useful in many applications, and examples of potential uses are given. © 1975, ACM. All rights reserved.},
   author = {Jon Louis Bentley},
   doi = {10.1145/361002.361007},
   issn = {15577317},
   issue = {9},
   journal = {Communications of the ACM},
   title = {Multidimensional Binary Search Trees Used for Associative Searching},
   volume = {18},
   year = {1975},
}

@article{Liu2006,
   abstract = {This paper is about non-approximate acceleration of high-dimensional nonparametric operations such as k nearest neighbor classifiers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly find the data points close to the query, but merely need to answer questions about the properties of that set of data points. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. This is applicable to many algorithms in nonparametric statistics, memory-based learning and kernel-based learning. But for clarity, this paper concentrates on pure k-NN classification. We introduce new ball-tree algorithms that on real-world data sets give accelerations from 2-fold to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include data sets with up to 106 dimensions and 105 records, and demonstrate non-trivial speed-ups while giving exact answers.},
   author = {Ting Liu and Andrew W. Moore and Alexander Gray},
   doi = {10.7551/mitpress/4908.003.0008},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   title = {New algorithms for efficient high-dimensional nonparametric classification},
   volume = {7},
   year = {2006},
}

@inproceedings{Amueller2014,
   abstract = {It is shown that for cuckoo hashing with a stash as proposed by Kirsch et al. (Proc. 16th European Symposium on Algorithms (ESA), pp. 611-622, Springer, Berlin, 2008) families of very simple hash functions can be used, maintaining the favorable performance guarantees: with constant stash size s the probability of a rehash is O(1/n s+1), the lookup time and the deletion time are O(s) in the worst case, and the amortized expected insertion time is O(s) as well. Instead of the full randomness needed for the analysis of Kirsch et al. and of Kutzelnigg (Discrete Math. Theor. Comput. Sci., 12(3):81-102, 2010) (resp. Θ(logn)-wise independence for standard cuckoo hashing) the new approach even works with 2-wise independent hash families as building blocks. Both construction and analysis build upon the work of Dietzfelbinger and Woelfel (Proc. 35th ACM Symp. on Theory of Computing (STOC), pp. 629-638, 2003). The analysis, which can also be applied to the fully random case, utilizes a graph counting argument and is much simpler than previous proofs. The results can be generalized to situations where the stash size is non-constant. © 2013 Springer Science+Business Media New York.},
   author = {Martin Aumüller and Martin Dietzfelbinger and Philipp Woelfel},
   doi = {10.1007/s00453-013-9840-x},
   issn = {14320541},
   issue = {3},
   journal = {Algorithmica},
   title = {Explicit and efficient hash families suffice for cuckoo hashing with a stash},
   volume = {70},
   year = {2014},
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@article{FAISS2017,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1702.08734},
  year={2017}
}

@generic{Prokhorenkova2019,
   abstract = {Graph-based approaches are empirically shown to be very successful for approximate nearest neighbor (ANN) search. However, there has been very little research on their theoretical guarantees. In this work, we consider both low-dimensional (d ≪ log n) and high-dimensional (d ≫ log n) regimes and rigorously analyze the performance of graph-based nearest neighbor algorithms when the dataset is uniformly distributed on a d-dimensional sphere. For both regimes, we provide the conditions which guarantee that a graph-based algorithm solves the ANN problem in just one iteration. In the low-dimensional regime, we also show that it is possible to solve the exact nearest neighbor problem. Finally, we discuss how the “small-world” property affects the performance of graph-based approaches.},
   author = {Liudmila Prokhorenkova},
   issn = {23318422},
   journal = {arXiv},
   title = {Graph-based nearest neighbor search: From practice to theory},
   year = {2019},
}

@inproceedings{Dong2011,
   abstract = {K-Nearest Neighbor Graph (K-NNG) construction is an iportant operation with many web related applications, including collaborative filtering, similarity search, and many others in data mining and machine learning. Existing methods for K-NNG construction either do not scale, or are specific to certain similarity measures. We present NN-Descent, a simple yet efficient algorithm for approximate K-NNG construction with arbitrary similarity measures. Our method is based on local search, has minimal space overhead and does not rely on any shared global index. Hence, it is especially suitable for large-scale applications where data structures need to be distributed over the network. We have shown with a variety of datasets and similarity measures that the proposed method typically converges to above 90% recall with each point comparing only to several percent of the whole dataset on average. Copyright © 2011 by the Association for Computing Machinery, Inc. (ACM).},
   author = {Wei Dong and Moses Charikar and Kai Li},
   doi = {10.1145/1963405.1963487},
   journal = {Proceedings of the 20th International Conference on World Wide Web, WWW 2011},
   title = {Efficient K-nearest neighbor graph construction for generic similarity measures},
   year = {2011},
}

@inproceedings{Indyk1998,
   abstract = {The nearest neighbor problem is given with the case of the d-dimensional Euclidean space where X = Rd under some lp norm being investigated. The low-dimensional case is well-solved, however, the issue of dealing with the curse of dimensionality is still a problem. Two algorithmic results for the approximate version that significantly improve the known bounds are presented. The preprocessing cost polynomial in n and d, and a truly sublinear query time and query time polynomial in log n and d, and only a mildly exponential preprocessing cost O(n)×O(1/ε)d are presented. By applying a classical geometric lemma on random projections, the first known algorithm with polynomial preprocessing and query time polynomial in d and log n is obtained.},
   author = {Piotr Indyk and Rajeev Motwani},
   doi = {10.4086/toc.2012.v008a014},
   issn = {07349025},
   journal = {Conference Proceedings of the Annual ACM Symposium on Theory of Computing},
   title = {Approximate nearest neighbors: Towards removing the curse of dimensionality},
   year = {1998},
}

@software{geopy,
   author = {Brian Beck, et. al.},
   abstract = {geopy is a Python client for several popular geocoding web services},
   title = {geopy},
   url = {https://github.com/geopy/geopy},
}

@inproceedings{Cohen2010,
   abstract = {In this paper we present a new problem, the fast set intersection problem, which is to preprocess a collection of sets in order to efficiently report the intersection of any two sets in the collection. In addition we suggest new solutions for the two-dimensional substring indexing problem and the document listing problem for two patterns by reduction to the fast set intersection problem. © 2010 Springer-Verlag.},
   author = {Hagai Cohen and Ely Porat},
   doi = {10.1007/978-3-642-12200-2_22},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Fast set intersection and two-patterns matching},
   volume = {6034 LNCS},
   year = {2010},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{Zhou2009,
   abstract = {Object localization based on radio frequency identification (RFID) technology has promising potentials. By combining localization with its identification capability, existing applications can be enhanced and new applications can be developed for this technology. This paper starts with an overview introducing the available technologies for localization with a focus on radio frequency based technologies. The existing and potential applications of RFID localization in various industries are then summarized. Moreover, RFID localization algorithms are reviewed, which can be categorized into multilateration, Bayesian inference, nearest-neighbor, proximity, and kernel-based learning methods. Also, we present a localization case study using passive RFID technology, and it shows that objects can be successfully localized using either multilateration or Bayesian inference methods. The survey also discusses the challenges and future research on RFID localization. © 2008 Springer Science+Business Media, LLC.},
   author = {Junyi Zhou and Jing Shi},
   doi = {10.1007/s10845-008-0158-5},
   issn = {09565515},
   issue = {6},
   journal = {Journal of Intelligent Manufacturing},
   title = {RFID localization algorithms and applications-A review},
   volume = {20},
   year = {2009},
}

@inproceedings{Zhang2017,
   abstract = {Localization has always been a topic of concern in transportation and construction management. Radio frequency identification (RFID) has promising potentials due to their practically unlimited identification capacity, lower power and light weight. The objective of this research is to develop localization algorithms using RFID technology and analyse which is possible algorithm indoor environment. To achieve this goal, this study derives multilateration and nearest-neighbour algorithms for localization. Different parameter including distance and RSSI between the antenna and the target tag is then calculated based on Received Signal Strength Indication and Phase Angle. To make the localization more accurate, this survey also analyse the localization results by using normal tags and Wireless Identification and Sensing Platform (WISP).},
   author = {Yipeng Zhang and Fan Zhang and Yang Wang and Yulin Ma and Honghai Li},
   doi = {10.1109/ICSCSE.2017.64},
   journal = {Proceedings - 2nd International Conference on Smart City and Systems Engineering, ICSCSE 2017},
   title = {Localization of Nearest-Neighbour and Multilateration Analyse Based on RFID},
   year = {2017},
}


@inproceedings{Tillquist2016,
author = {Tillquist, Richard C. and Lladser, Manuel E.},
title = {Metric-Space Positioning Systems (MPS) for Machine Learning},
year = {2016},
isbn = {9781450342254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.echo.louisville.edu/10.1145/2975167.2985641},
doi = {10.1145/2975167.2985641},
abstract = {Many machine learning techniques such as k-nearest neighbors (KNNs) and support vector machines (SVMs) require examples to be mapped to numerical feature vectors. Principal coordinate analysis (PCoA) accomplishes this by mapping a set of n examples to n points in Rn-1. However, learning from these high-dimensional vectors may require an astronomically large number of examples. Here we present an intuitive, novel method for uniquely representing sequences of symbolic features with the fewest dimensions via "multi-lateration" (i.e. the selection of a minimal feature set that uniquely distinguishes all examples under a reference metric). We show that the problem of determining a minimal multilateration set is NP-complete in general, and present a randomized algorithm for finding close to optimal subsets. As proof of concept, we apply multilateration to learn 12-mers centered at intron-exon boundaries using human annotated examples. We compare results using features derived in several different ways and an array of machine learning techniques, some of which can handle symbolic features directly and some of which cannot. Our experiments indicate that multilateration improves performance of non-symbolic classification techniques without significantly altering performance using other techniques.},
booktitle = {Proceedings of the 7th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {479},
numpages = {1},
keywords = {Dimension Reduction, NP-Complete, Multilateration, Sequence Features},
location = {Seattle, WA, USA},
series = {BCB '16}
}

@article{Strohmeier2018,
   abstract = {In this paper, we argue that current state-of-the-art methods of aircraft localization such as multilateration are insufficient, in particular for modern crowdsourced air traffic networks with random, unplanned deployment geometry. We propose an alternative, a grid-based localization approach using the k-nearest neighbor (k -NN) algorithm, to deal with the identified shortcomings. Our proposal does not require any changes to the existing air traffic protocols and transmitters, and is easily implemented using only low-cost, commercial-off-the-shelf hardware. Using an algebraic multilateration algorithm for comparison, we evaluate our approach using real-world flight data collected with our collaborative sensor network OpenSky. We quantify its effectiveness in terms of aircraft location accuracy, surveillance coverage, and the verification of false position data. Our results show that the grid-based k-NN approach can increase the effective air traffic surveillance coverage compared to multilateration by a factor of up to 2.5. As it does not suffer from dilution of precision to the same extent, it is more robust in noisy environments and performs better in pre-existing, unplanned receiver deployments. We further find that the mean aircraft location accuracy can be increased by up to 41% in comparison with multilateration while also being able to pinpoint the origin of potential spoofing attacks conducted from the ground.},
   author = {Martin Strohmeier and Ivan Martinovic and Vincent Lenders},
   doi = {10.1109/TAES.2018.2797760},
   issn = {00189251},
   issue = {3},
   journal = {IEEE Transactions on Aerospace and Electronic Systems},
   keywords = {Air traffic control (ATC),crowdsourced networks,k-nearest neighbors (k-NN),wireless localization},
   month = {6},
   pages = {1519-1529},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A k-NN-Based Localization Approach for Crowdsourced Air Traffic Communication Networks},
   volume = {54},
   year = {2018},
}


@article{LINARES20201,
title = {Effects of number of digits in large-scale multilateration},
journal = {Precision Engineering},
volume = {64},
pages = {1-6},
year = {2020},
issn = {0141-6359},
doi = {https://doi.org/10.1016/j.precisioneng.2020.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0141635919308591},
author = {Jean Marc Linares and Santiago Arroyave-Tobon and José Pires and Jean Michel Sprauel},
keywords = {Multilateration, Tracking interferometer, Uncertainty, Floating-point, Numerical stability},
abstract = {Since many years ago, multilateration has been used in precision engineering notably in machine tool and coordinate measuring machine calibration. This technique needs, first, the use of laser trackers or tracking interferometers, and second, the use of nonlinear optimization algorithms to determine point coordinates. Research works have shown the influence of the experimental configuration on measure precision in multilateration. However, the impact of floating-point precision in computations on large-scale multilateration precision has not been addressed. In this work, the effects of numerical errors (rounding and cancellation effects) due to floating-point precision (number of digits) were studied. In order to evaluate these effects in large-scale multilateration, a multilateration measurement system was simulated. This protocol is illustrated with a case study where large distances (≤20 m) between pairs of target points were simulated. The results show that the use of multi-precision libraries is recommended to control the propagation of uncertainties during the multilateration computation.}
}


@inproceedings{Singh2015,
   abstract = {Recent developments in micro electro mechanical systems (MEMS) technology and wireless communication have propelled the growing applications of wireless sensor networks (WSNs). Wireless sensor network is comprised of large number of small and cheap devices known as sensors. One of the important functions of sensor network is collection and forwarding of data. In most of the applications, it is of much interest to find out the location of the data. This type of information can be obtained by use of localization techniques. So node localization is very crucial to find out the position of node with the help of localization algorithms. Hence, node localization becomes one of the fundamental challenges in WSNs. We make the rigorous reviews on different schemes of localization in sensor networks. On the basis of range measurements, the localization schemes can be broadly classified in two categories such as: range based and range free schemes. The cost and hardware limitation on sensing node preclude the use of range based localization schemes. In most of the sensor network application coarse accuracy is sufficient so range free localization schemes are considered as a substitute to range based schemes. In this paper, the detailed study has been carried out to understand and select the best range free localization algorithm for WSNs. At the end some issues are discussed for future research in the area of localization techniques for WSNs.},
   author = {Santar Pal Singh and S. C. Sharma},
   doi = {10.1016/j.procs.2015.07.357},
   issn = {18770509},
   journal = {Procedia Computer Science},
   keywords = {accuracy,localization algorithms,range,range free localization,wireless sensor networks},
   month = {1},
   pages = {7-16},
   publisher = {Elsevier},
   title = {Range Free Localization Techniques in Wireless Sensor Networks: A Review},
   volume = {57},
   year = {2015},
}

@article{Kazel1972,
   abstract = {An airborne radar concept is described which would achieve high-resolution mapping of terrain, even with wide-beam antennas, by utilizing a narrow pulse-width. Successive high-resolution range rings, superimposed in a suitable storage medium, cause the terrain image to build up as the aircraft flies by.},
   author = {Sidney Kazel},
   doi = {10.1109/PROC.1972.8886},
   issn = {00189219},
   issue = {10},
   journal = {Proceedings of the IEEE},
   title = {MULTILATERATION RADAR.},
   volume = {60},
   year = {1972},
}

@article{Wasserstein1989,
   author = {Ronald L. Wasserstein and Malvin H. Kalos and Paula A. Whitlock},
   doi = {10.2307/1268841},
   issn = {00401706},
   issue = {2},
   journal = {Technometrics},
   title = {Monte Carlo Methods, Volume 1: Basics},
   volume = {31},
   year = {1989},
}


@article{Lee1975,
   abstract = {This paper describes a novel procedure for determing the accuracy of hyperbolic multilateration systems. Basically, the procedure links the conventional accuracy measures (e.g., GDOP) to the moments and products of inertia of a mass configuration that is easily derived from the system geometry. Thus, the problem of determining accuracy measures is reduced to that of calculating simple moments and products of inertia. The insight provided by the procedure makes it possible to derive a variety of useful approximations for GDOP and other accuracy measures. Copyright © 1975 by The Institute of Electrical and Electronics Engineers, Inc.},
   author = {Harry B. Lee},
   doi = {10.1109/TAES.1975.308023},
   issn = {00189251},
   issue = {1},
   journal = {IEEE Transactions on Aerospace and Electronic Systems},
   title = {A Novel Procedure for Assessing the Accuracy of Hyperbolic Multilateration Systems},
   volume = {AES-11},
   year = {1975},
}


@article{Abel1991,
   abstract = {The existence and uniqueness of positions computed from Global Positioning System (GPS) pseudorange (PR) measurements is studied. Contrary to several recent claims [2, 3], in the case of n = 4 satellites a fix may not exist, and, if a fix exists, it is not guaranteed to be unique. In the case of n -5 satellites a unique fix is assured, except in certain degenerate cases such as coplanar satellites. Finally, an alternate formulation of the direct n = 4 PR to three-space position solutions [2, 3] is presented, and simple tests for existence and uniqueness are derived. © 1991 IEEE},
   author = {Jonathan S. Abel and James W. Chaffee},
   doi = {10.1109/7.104271},
   issn = {00189251},
   issue = {6},
   journal = {IEEE Transactions on Aerospace and Electronic Systems},
   title = {Existence And Uniqueness Of Gps Solutions},
   volume = {27},
   year = {1991},
}


@article{Kaplan2018,
   abstract = {Before Columbus there was Eratosthenes: 'inventor' of the discipline of geography as it is known today and the first person to calculate the circumference of the Earth. There was Alexander the Great: the man who sought to reach the very ends of the known world and whose empire spanned three continents, from the Ionian Sea to the Himalayas. And there was Strabo: author of the Geographica, a 17-volume encyclopaedia of geographical knowledge which expounded the definition, history and mathematics of geography, both physical and human, and charted the limits of the habitable world. These are but three of a multitude of figures whose contributions to geography and human knowledge have left an enduring and indelible legacy.In this, the first major study of ancient geography and geographers to be published in English for over 60 years, Duane Roller offers a comprehensive account of these ancient pioneers and the frontiers that defined their world.From the Bronze Age to Late Antiquity, Roller maps the development of geographical scholarship from its incipient beginnings in the literature of Hesiod, Homer, Herodotus and the tragedians through to the learned compendia of Posidonius and Strabo - and the scientific discoveries of Pythagoras, Eratosthenes and Euclid that made it all possible.},
   author = {Philip Kaplan},
   doi = {10.1080/2325548x.2018.1402263},
   issn = {2325-548X},
   issue = {1},
   journal = {The AAG Review of Books},
   title = {Ancient Geography: The Discovery of the World in Classical Greece and Rome},
   volume = {6},
   year = {2018},
}


@book{VanBrummelen2012,
   abstract = {Spherical trigonometry was at the heart of astronomy and ocean-going navigation for two millennia. The discipline was a mainstay of mathematics education for centuries, and it was a standard subject in high schools until the 1950s. Today, however, it is rarely taught.Heavenly Mathematicstraces the rich history of this forgotten art, revealing how the cultures of classical Greece, medieval Islam, and the modern West used spherical trigonometry to chart the heavens and the Earth. Glen Van Brummelen explores this exquisite branch of mathematics and its role in ancient astronomy, geography, and cartography; Islamic religious rituals; celestial navigation; polyhedra; stereographic projection; and more. He conveys the sheer beauty of spherical trigonometry, providing readers with a new appreciation for its elegant proofs and often surprising conclusions. Heavenly Mathematicsis illustrated throughout with stunning historical photographs and informative drawings and diagrams that have been used to teach the subject in the past. This unique compendium also features easy-to-use appendixes as well as exercises at the end of each chapter that originally appeared in textbooks from the eighteenth to the early twentieth centuries. © 2013 by Princeton University Press. All Rights Reserved.},
   author = {Glen Van Brummelen},
   doi = {10.33137/aestimatio.v11i0.26065},
   issn = {1549-4470},
   journal = {Heavenly Mathematics: The Forgotten Art of Spherical Trigonometry},
   title = {Heavenly mathematics: The forgotten art of spherical trigonometry},
   year = {2012},
}


@article{Chen2018,
   abstract = {Many modern methods for prediction leverage nearest neighbor search to find past training examples most similar to a test example, an idea that dates back in text to at least the 11th century and has stood the test of time. This monograph aims to explain the success of these methods, both in theory, for which we cover foundational nonasymptotic statistical guarantees on nearest-neighbor-based regression and classification, and in practice, for which we gather prominent methods for approximate nearest neighbor search that have been essential to scaling prediction systems reliant on nearest neighbor analysis to handle massive datasets. Furthermore, we discuss connections to learning distances for use with nearest neighbor methods, including how random decision trees and ensemble methods learn nearest neighbor structure, as well as recent developments in crowdsourcing and graphons. In terms of theory, our focus is on nonasymptotic statistical guarantees, which we state in the form of how many training data and what algorithm parameters ensure that a nearest neighbor prediction method achieves a user-specified error tolerance. We begin with the most general of such results for nearest neighbor and related kernel regression and classification in general metric spaces. In such settings in which we assume very little structure, what enables successful prediction is smoothness in the function being estimated for regression, and a low probability of landing near the decision boundary for classification. In practice, these conditions could be difficult to verify empirically for a real dataset. We then cover recent theoretical guarantees on nearest neighbor prediction in the three case studies of time series forecasting, recommending products to people over time, and delineating human organs in medical images by looking at image patches. In these case studies, clustering structure, which is easier to verify in data and more readily interpretable by practitioners, enables successful prediction.},
   author = {George H. Chen and Devavrat Shah},
   doi = {10.1561/2200000064},
   issn = {19358245},
   issue = {5-6},
   journal = {Foundations and Trends in Machine Learning},
   title = {Explaining the success of nearest neighbor methods in prediction},
   volume = {10},
   year = {2018},
}


@article{PJA2010,
   abstract = {It is well known that high-dimensional nearest neighbor retrieval is very expensive. Dramatic performance gains are obtained using approximate search schemes, such as the popular Locality-Sensitive Hashing (LSH). Several extensions have been proposed to address the limitations of this algorithm, in particular, by choosing more appropriate hash functions to better partition the vector space. All the proposed extensions, however, rely on a structured quantizer for hashing, poorly fitting real data sets, limiting its performance in practice. In this paper, we compare several families of space hashing functions in a real setup, namely when searching for high-dimension SIFT descriptors. The comparison of random projections, lattice quantizers, k-means and hierarchical k-means reveal that unstructured quantizer significantly improves the accuracy of LSH, as it closely fits the data in the feature space. We then compare two querying mechanisms introduced in the literature with the one originally proposed in LSH, and discuss their respective merits and limitations. © 2010 Elsevier B.V. All rights reserved.},
   author = {Loïc Paulevé and Hervé Jégou and Laurent Amsaleg},
   doi = {10.1016/j.patrec.2010.04.004},
   issn = {01678655},
   issue = {11},
   journal = {Pattern Recognition Letters},
   title = {Locality sensitive hashing: A comparison of hash function types and querying mechanisms},
   volume = {31},
   year = {2010},
}


@inproceedings{Fu2018,
   abstract = {Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial scenario of Taobao (Alibaba Group) and has been integrated into their billion-scale search engine.},
   author = {Cong Fu and Chao Xiang and Changxu Wang and Deng Cai},
   doi = {10.14778/3303753.3303754},
   issn = {21508097},
   issue = {5},
   journal = {Proceedings of the VLDB Endowment},
   title = {Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph},
   volume = {12},
   year = {2018},
}

@article{Chen2011,
   abstract = {Similarity search is important in information retrieval applications where objects are usually represented as vectors of high dimensionality. This leads to the increasing need for supporting the indexing of high-dimensional data. On the other hand, indexing structures based on space partitioning are powerless because of the well-known "curse of dimensionality". Linear scan of the data with approximation is more efficient in the high-dimensional similarity search. However, approaches so far have concentrated on reducing I/O, and ignored the computation cost. For an expensive distance function such as Lp norm with fractional p, the computation cost becomes the bottleneck. We propose a new technique to address expensive distance functions by "indexing the function" by pre-computing some key values of the function once. Then, the values are used to develop the upper/lower bounds of the distance between a data vector and the query vector. The technique is extremely efficient since it avoids most of the distance function computations; moreover, it does not involve any extra secondary storage because no index is constructed and stored. The efficiency is confirmed by cost analysis, as well as experiments on synthetic and real data. © 2010 Springer-Verlag London Limited.},
   author = {Hanxiong Chen and Jianquan Liu and Kazutaka Furuse and Jeffrey Xu Yu and Nobuo Ohbo},
   doi = {10.1007/s10115-010-0303-2},
   issn = {02191377},
   issue = {2},
   journal = {Knowledge and Information Systems},
   title = {Indexing expensive functions for efficient multi-dimensional similarity search},
   volume = {27},
   year = {2011},
}

@inproceedings{Weber1998,
   abstract = {For similarity search in high-dimensional vector spaces (or ``HDVSs''), researchers have proposed a number of new methods (or adaptations of existing methods) based, in the main, on data-space partitioning. However, the performance of these methods generally degrades as dimensionality increases. Although this phenomenon-known as the ``dimensional curse''-is well known, little or no quantitative analysis of the phenomenon is available. In this paper, we provide a detailed analysis of partitioning and clustering techniques for similarity search in HDVSs. We show formally that these methods exhibit linear complexity at high dimensionality, and that existing methods are outperformed on average by a simple sequential scan if the number of dimensions exceeds around 10. Consequently, we come up with an alternative organization based on approximations to make the unavoidable sequential scan as fast as possible. We describe a simple vector approximation scheme, called VA-file, and report on an experimental evaluation of this and of two tree-based index methods (an R -tree and an X-tree).},
   author = {Roger Weber and Hans J Schek and Stephen Blott},
   journal = {Proceedings of the 24rd International Conference on Very Large Data Bases},
   title = {A Quantitative Analysis and Performance Study for Similarity-Search Methods in High-Dimensional Spaces},
   year = {1998},
   
}

@article{Morton1966,
   abstract = {A coordinate system for use with map or survey data is introduced and developed.  Although this system can be applied to any sized area on any part of the Earth's surface, particular reference is made to Canada.  The problems of using data spanning such a large area are discussed and a new technique of file sequencing is introduced to counter those problems.  Some of the properties of such a file sequence are used to demonstrate the flexibility and power to be gained by use of this technique.},
   author = {Morton, G. M.},
   journal = {International Business Machines},
   title = {A Computer Oriented Geodetic Data Base and a New Technique in File Sequencing},
   year = {1966},
}